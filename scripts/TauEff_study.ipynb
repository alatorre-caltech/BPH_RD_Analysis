{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "trgMu_pt_region = 'mid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, pickle\n",
    "from glob import glob\n",
    "sys.path.append('../lib')\n",
    "import itertools\n",
    "import json\n",
    "from IPython.display import IFrame, Image, display\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T01:36:12.149848Z",
     "start_time": "2019-05-14T01:36:11.232339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.12/07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.interpolate import interp1d\n",
    "from array import array\n",
    "\n",
    "import uproot as ur\n",
    "import ROOT as rt\n",
    "rt.gErrorIgnoreLevel = rt.kError\n",
    "rt.RooMsgService.instance().setGlobalKillBelow(rt.RooFit.ERROR)\n",
    "import root_numpy as rtnp\n",
    "\n",
    "from analysis_utilities import drawOnCMSCanvas, extarct, extarct_multiple, createSel\n",
    "from histo_utilities import create_TH1D, create_TH2D, std_color_list\n",
    "from cebefo_style import Set_2D_colz_graphics\n",
    "from gridVarQ2Plot import plot_gridVarQ2\n",
    "from progressBar import ProgressBar\n",
    "\n",
    "import CMS_lumi, tdrstyle\n",
    "tdrstyle.setTDRStyle()\n",
    "CMS_lumi.writeExtraText = 1\n",
    "\n",
    "CMS_lumi.extraText = \"Simulation Preliminary\"\n",
    "\n",
    "donotdelete = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "binning = {\n",
    "    'q2'      : array('d', [-2, 1.5, 4, 6, 12]),\n",
    "    'M2_miss' : [\n",
    "        array('d', [-2.5] + list(np.arange(-1.8, 0.8, 0.4)) + [6] ),\n",
    "        array('d', [-2.5] + list(np.arange(-1.8, 1.6, 0.4)) + [6] ),\n",
    "        array('d', [-2.5] + list(np.arange(-1.8, 3.0, 0.4)) + [6] ),\n",
    "        array('d', [-2.5] + list(np.arange(-1.8, 5.2, 0.4)) + [6] ),\n",
    "    ],\n",
    "    'Est_mu'  : [\n",
    "        array('d', [0.5] + list(np.arange(0.8, 2.3, 0.1)) + [2.5] ),\n",
    "        array('d', [0.5] + list(np.arange(0.8, 2.5, 0.1)) + [2.5] ),\n",
    "        [20, 0.50, 2.500],\n",
    "        [20, 0.50, 2.500],\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create histograms file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '../data/cmsMC_private/BPH_Tag-'\n",
    "file_loc = {\n",
    "'mu'   : base+'B0_MuNuDmst-pD0bar-kp_13TeV-pythia8_Hardbbbar_PTFilter5_0p0-evtgen_ISGW2_PU20_10-2-3/ntuples_B2DstMu/out_CAND_*.root',\n",
    "'tau'  : base+'B0_TauNuDmst-pD0bar-kp-t2mnn_pythia8_Hardbbbar_PTFilter5_0p0-evtgen_ISGW2_PU20_10-2-3/ntuples_B2DstMu/out_CAND_*.root',\n",
    "'Hc'   : base+'B0_DmstHc-pD0bar-kp-Hc2mu_13TeV-pythia8_Hardbbbar_PTFilter5_0p0-evtgen_PU20_10-2-3/ntuples_B2DstMu/out_CAND_*.root',\n",
    "'Dstst': base+'Bp_MuNuDstst_DmstPi_13TeV-pythia8_Hardbbbar_PTFilter5_0p0-evtgen_ISGW2_PU20_10-2-3/ntuples_B2DstMu/out_CAND_*.root'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_selection(ev):        \n",
    "    if trgMu_pt_region == 'high':\n",
    "        aux = ev.trgMu_HLT_Mu12_IP6 == 1\n",
    "        aux &= ev.trgMu_pt > 12.1\n",
    "    elif trgMu_pt_region == 'mid':\n",
    "        aux = ev.trgMu_HLT_Mu12_IP6 == 0\n",
    "        aux &= ev.trgMu_HLT_Mu9_IP6 == 1\n",
    "        aux &= ev.trgMu_pt > 9.1\n",
    "        aux &= ev.trgMu_pt < 11.9\n",
    "    \n",
    "    aux &= abs(ev.trgMu_eta) < 2.4\n",
    "    aux &= ev.trgMu_sigdxy > 7\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_selection(j, ev, skip_cut=None):\n",
    "    aux = True\n",
    "    if not skip_cut==0: aux &= ev.pval_piK[j] > 0.1\n",
    "    if not skip_cut==1: aux &= ev.K_pt[j] > 0.8\n",
    "    if not skip_cut==2: aux &= abs(ev.K_eta[j]) < 2.5\n",
    "    if not skip_cut==3: aux &= ev.pi_pt[j] > 0.8\n",
    "    if not skip_cut==4: aux &= abs(ev.pi_eta[j]) < 2.5\n",
    "    if not skip_cut==5: aux &= abs(ev.mass_piK[j] - 1.86) < 0.04\n",
    "    if not aux: \n",
    "        return False\n",
    "\n",
    "    if not skip_cut==6: aux = ev.pis_pt[j] > 0.5\n",
    "    if not skip_cut==7: aux &= ev.pval_D0pis[j] > 0.1\n",
    "    if not skip_cut==8: aux &= abs(ev.mass_D0pis[j] - 2.01) < 0.03\n",
    "    if not skip_cut==9: aux &= ev.sigdxy_pis_PV[j] > 2\n",
    "    if not skip_cut==10: aux &= ev.mass_D0pis[j] < ev.mass_piK[j] + 0.16\n",
    "    if not skip_cut==11: aux &= ev.pval_D0pismu[j] > 0.1\n",
    "    if not skip_cut==12: aux &= ev.cos_D0pismu_PV[j] > 0.99\n",
    "    if not skip_cut==13: aux &= ev.q2_D0pismu[j] > binning['q2'][0]\n",
    "    if not skip_cut==14: aux &= ev.q2_D0pismu[j] < binning['q2'][-1]\n",
    "    if not skip_cut==15: aux &= ev.mass_D0pismu[j] < 7.\n",
    "    if not aux: \n",
    "        return False\n",
    "    \n",
    "    if not ev.nTksAdd[j] == 0:\n",
    "        idx_st = int(np.sum(ev.nTksAdd[:j]))\n",
    "        # DEBUG controlla questa linea la prossima volta che fai le ntuple\n",
    "        idx_stop = min(ev.tksAdd_massVis.size(), int(idx_st + ev.nTksAdd[j]))\n",
    "        for jj in range(idx_st, idx_stop):\n",
    "            if ev.tksAdd_massVis[jj] < 5.28:\n",
    "                return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dSet(inputs, serial=False):\n",
    "    n = inputs[0]\n",
    "    filepath = inputs[1]\n",
    "    skip_cut = inputs[2]\n",
    "    \n",
    "    print n\n",
    "    fskimmed_name = '../data/_root/skimmed4side/' + n + '.root'\n",
    "    if os.path.isfile(fskimmed_name) and not n in recreate:\n",
    "        dSet[n] = rtnp.root2array(fskimmed_name)\n",
    "    else:\n",
    "        tree = rt.TChain('outA/Tevts')\n",
    "        for fn in glob(filepath):\n",
    "            tree.Add(fn)\n",
    "        N_cand_in = tree.GetEntries()\n",
    "        print n, ': Number of candidate events:', N_cand_in\n",
    "    \n",
    "        if serial:\n",
    "            pb = ProgressBar(maxEntry=tree.GetEntries())\n",
    "        else:\n",
    "            maxEntry = float(tree.GetEntries())\n",
    "            perc = int(maxEntry*0.1)\n",
    "        N_accepted_cand = []\n",
    "        output = []\n",
    "        for i_ev, ev in enumerate(tree):\n",
    "            if serial:\n",
    "                pb.show(i_ev)\n",
    "            elif i_ev % perc == 0:\n",
    "                print n, ': {:.0f}%'.format(100*(i_ev+1)/maxEntry)\n",
    "            N_acc = 0\n",
    "            \n",
    "            if not trigger_selection(ev): \n",
    "                continue\n",
    "                \n",
    "            for j in range(ev.pval_piK.size()):\n",
    "                if not candidate_selection(j, ev, skip_cut=skip_cut):\n",
    "                    continue\n",
    "                    \n",
    "                N_acc += 1\n",
    "                aux = (ev.q2_D0pismu[j], ev.Est_mu_D0pismu[j], ev.M2_miss_D0pismu[j])\n",
    "                aux += (ev.trgMu_pt, ev.trgMu_sigdxy)\n",
    "                aux += (ev.B_D0pismu_pt[j], ev.B_D0pismu_eta[j])\n",
    "                if not n == 'data':\n",
    "                    aux += (ev.MC_B_pt, ev.MC_B_eta)\n",
    "                if n in ['tau', 'mu']:\n",
    "                    aux += (\n",
    "                           ev.wh_CLNCentral,\n",
    "                           ev.wh_CLNR0Down,\n",
    "                           ev.wh_CLNR0Up,\n",
    "                           ev.wh_CLNR1Down,\n",
    "                           ev.wh_CLNR1Up,\n",
    "                           ev.wh_CLNR2Down,\n",
    "                           ev.wh_CLNR2Up,\n",
    "                           ev.wh_CLNRhoSqDown,\n",
    "                           ev.wh_CLNRhoSqUp,\n",
    "                    )\n",
    "                \n",
    "                output.append(aux)\n",
    "            \n",
    "            if N_acc > 0:\n",
    "                N_accepted_cand.append(N_acc)\n",
    "        \n",
    "        print n, ': Loop done'\n",
    "        leafs_names = ['q2', 'Est_mu', 'M2_miss', 'B_pt', 'B_eta', 'trgMu_pt', 'trgMu_sigdxy']\n",
    "        if not n == 'data':\n",
    "            leafs_names += ['MC_B_pt', 'MC_B_eta']\n",
    "        if n in ['tau', 'mu']:\n",
    "            leafs_names += ['wh_CLNCentral', 'wh_CLNR0Down', 'wh_CLNR0Up', 'wh_CLNR1Down', 'wh_CLNR1Up', 'wh_CLNR2Down', 'wh_CLNR2Up', 'wh_CLNRhoSqDown', 'wh_CLNRhoSqUp']\n",
    "\n",
    "        dtypes = []\n",
    "        for nl in leafs_names:\n",
    "            dtypes.append((nl, np.float32))\n",
    "            \n",
    "        dSet[n] = np.array(output, dtype=dtypes)\n",
    "        if not os.path.isdir(os.path.dirname(fskimmed_name)):\n",
    "            os.makedirs(os.path.dirname(fskimmed_name))\n",
    "        rtnp.array2root(dSet[n], fskimmed_name, treename='Tevts', mode='RECREATE')\n",
    "        \n",
    "    aux = dSet[n].dtype.fields.keys()[0]\n",
    "    print n, ': Events:', dSet[n][aux].shape[0]\n",
    "    print ''\n",
    "    return dSet[n][aux].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[None] + range(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tau\n",
      "tau : Number of candidate events: 81453\n",
      "[####################]  100% - Tot. time: 22.4 s\n",
      "tau : Loop done\n",
      "tau : Events: 9311\n",
      "\n",
      "tau\n",
      "tau : Number of candidate events: 81453\n",
      "[####################]  100% - Tot. time: 22.8 s\n",
      "tau : Loop done\n",
      "tau : Events: 9802\n",
      "\n",
      "tau\n",
      "tau : Number of candidate events: 81453\n",
      "[####################]  100% - Tot. time: 22.5 s\n",
      "tau : Loop done\n",
      "tau : Events: 9423\n",
      "\n",
      "tau\n",
      "tau : Number of candidate events: 81453\n",
      "[####################]  100% - Tot. time: 22.4 s\n",
      "tau : Loop done\n",
      "tau : Events: 9311\n",
      "\n",
      "tau\n",
      "tau : Number of candidate events: 81453\n",
      "[####################]  100% - Tot. time: 22.6 s\n",
      "tau : Loop done\n",
      "tau : Events: 9533\n",
      "\n",
      "tau\n",
      "tau : Number of candidate events: 81453\n",
      "[####################]  100% - Tot. time: 22.4 s\n",
      "tau : Loop done\n",
      "tau : Events: 9311\n",
      "\n",
      "tau\n",
      "tau : Number of candidate events: 81453\n",
      "[####################]  100% - Tot. time: 22.3 s\n",
      "tau : Loop done\n",
      "tau : Events: 9311\n",
      "\n",
      "tau\n",
      "tau : Number of candidate events: 81453\n",
      "[####################]  100% - Tot. time: 22.4 s\n",
      "tau : Loop done\n",
      "tau : Events: 10038\n",
      "\n",
      "tau\n",
      "tau : Number of candidate events: 81453\n",
      "[####################]  100% - Tot. time: 22.4 s\n",
      "tau : Loop done\n",
      "tau : Events: 9615\n",
      "\n",
      "tau\n",
      "tau : Number of candidate events: 81453\n",
      "[####################]  100% - Tot. time: 22.4 s\n",
      "tau : Loop done\n",
      "tau : Events: 9663\n",
      "\n",
      "tau\n",
      "tau : Number of candidate events: 81453\n",
      "[####################]  100% - Tot. time: 22.4 s\n",
      "tau : Loop done\n",
      "tau : Events: 9311\n",
      "\n",
      "tau\n",
      "tau : Number of candidate events: 81453\n",
      "[####################]  100% - Tot. time: 22.1 s\n",
      "tau : Loop done\n",
      "tau : Events: 9315\n",
      "\n",
      "tau\n",
      "tau : Number of candidate events: 81453\n",
      "[####################]  100% - Tot. time: 22.6 s\n",
      "tau : Loop done\n",
      "tau : Events: 9886\n",
      "\n",
      "tau\n",
      "tau : Number of candidate events: 81453\n",
      "[####################]  100% - Tot. time: 22.4 s\n",
      "tau : Loop done\n",
      "tau : Events: 9323\n",
      "\n",
      "tau\n",
      "tau : Number of candidate events: 81453\n",
      "[####################]  100% - Tot. time: 22.4 s\n",
      "tau : Loop done\n",
      "tau : Events: 9397\n",
      "\n",
      "tau\n",
      "tau : Number of candidate events: 81453\n",
      "[####################]  100% - Tot. time: 22.4 s\n",
      "tau : Loop done\n",
      "tau : Events: 9311\n",
      "\n",
      "tau\n",
      "tau : Number of candidate events: 81453\n",
      "[####################]  100% - Tot. time: 22.6 s\n",
      "tau : Loop done\n",
      "tau : Events: 9311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dSet = {}\n",
    "recreate = file_loc.keys()\n",
    "# inputs = [[n, fp] for n, fp in file_loc.iteritems()]\n",
    "inputs = [['tau', file_loc['tau'], i] for i in [None] + range(16)]\n",
    "\n",
    "# p = Pool(len(inputs))\n",
    "# out = p.map(create_dSet, inputs) \n",
    "\n",
    "res = []\n",
    "for i in inputs:\n",
    "    out = create_dSet(i, serial=True)\n",
    "    res.append(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "print res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on is hat remained from previous script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run scripts/generation_breakdown_crab.ipynb to updated this file\n",
    "dic_MCeff = pickle.load(open('../data/SignalRegionMC_efficiencies_PU20.pickle', 'rb'))\n",
    "\n",
    "# This is always good unless one of the EvtGen card is changed\n",
    "dic_decayBR = pickle.load(open('../data/forcedDecayChannelsFactors.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, d in dic_MCeff.iteritems():\n",
    "    d['analysisSelEff'] = dSet[n]['q2'].shape[0]/float(d['TotCand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTCalibration:\n",
    "    def __init__(self, calibration_dic=None, calibration_file=None):\n",
    "        if not calibration_dic is None:\n",
    "            self.calibration_dic = calibration_dic\n",
    "            d = calibration_dic\n",
    "        elif not calibration_file is None:\n",
    "            d = {}\n",
    "            lines = open(calibration_file, 'r').readlines()\n",
    "            keys = lines[0][1:-1].split('\\t')\n",
    "            for k in keys: d[k] = []\n",
    "            \n",
    "            for l in lines[1:]:\n",
    "                l = l[:-1]\n",
    "                v = l.split('\\t')\n",
    "                for i in range(len(v)):\n",
    "                    d[keys[i]].append(float(v[i]))\n",
    "            \n",
    "            self.calibration_dic = d\n",
    "        else:\n",
    "            raise\n",
    "        \n",
    "        self.f = {}\n",
    "        self.f['C'] = interp1d(d['pt'], d['w'], \n",
    "                               fill_value=(d['w'][0], d['w'][-1]),\n",
    "                               bounds_error=False,\n",
    "                               kind='cubic'\n",
    "                                                )\n",
    "        self.f['Up'] = interp1d(d['pt'], d['wUp'], \n",
    "                               fill_value=(d['wUp'][0], d['wUp'][-1]),\n",
    "                               bounds_error=False,\n",
    "                               kind='cubic'\n",
    "                                                )\n",
    "        self.f['Down'] = interp1d(d['pt'], d['wDown'], \n",
    "                               fill_value=(d['wDown'][0], d['wDown'][-1]),\n",
    "                               bounds_error=False,\n",
    "                               kind='cubic'\n",
    "                                                )\n",
    "fname = '../data/calibration/B02JPsiKst_Mu12_pt.txt'\n",
    "if trgMu_pt_region == 'mid':\n",
    "    fname = '../data/calibration/B02JPsiKst_Mu9_pt.txt'\n",
    "cal_pT = PTCalibration(calibration_file=fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "histo = {}\n",
    "for i_q2 in range(len(binning['q2'])-1):\n",
    "    q2_l = binning['q2'][i_q2]\n",
    "    q2_h = binning['q2'][i_q2 + 1]\n",
    "\n",
    "    for var in ['M2_miss', 'Est_mu']:\n",
    "        cat_name = var+'_q2bin'+str(i_q2)\n",
    "        histo[cat_name] = {}\n",
    "        for k,d in dSet.iteritems():          \n",
    "            q2_bin = np.logical_and(d['q2'] > q2_l, d['q2'] < q2_h)\n",
    "            \n",
    "            if k == 'data':\n",
    "                histo[cat_name][k] = create_TH1D(d[var][q2_bin], \n",
    "                                                 name='data_obs', title='Data Obs',\n",
    "                                                 binning=binning[var][i_q2],\n",
    "                                                 opt='underflow,overflow'\n",
    "                                                )\n",
    "            elif k in ['mu', 'tau']:\n",
    "                for k_wpT in ['C', 'Up', 'Down']:\n",
    "                    pFF_list = ['Central']\n",
    "                    if k_wpT == 'C':\n",
    "                        pFF_list += ['R0', 'R1', 'R2', 'RhoSq']\n",
    "                    for n_pFF in pFF_list:\n",
    "                        var_pFF_list = [''] if n_pFF == 'Central' else ['Up', 'Down']\n",
    "                        for var_pFF in var_pFF_list:\n",
    "                            h_name = k\n",
    "                            if k_wpT != 'C':\n",
    "                                h_name += '__B0pT' + k_wpT\n",
    "                            elif n_pFF != 'Central':\n",
    "                                h_name += '__B2DstCLN' + n_pFF + var_pFF\n",
    "\n",
    "                            w = cal_pT.f[k_wpT](d['MC_B_pt'])\n",
    "                            w *= d['wh_CLN'+n_pFF+var_pFF]\n",
    "                            norm = np.sum(w)\n",
    "                            w = w[q2_bin]\n",
    "                            \n",
    "                            h = create_TH1D(d[var][q2_bin], name=h_name, title=h_name, \n",
    "                                            binning=binning[var][i_q2], \n",
    "                                            opt='underflow,overflow',\n",
    "                                            weights=w,\n",
    "                                            scale_histo=1./norm,\n",
    "                                           )\n",
    "\n",
    "                            histo[cat_name][h_name] = h\n",
    "            elif k in ['Hc']:\n",
    "                for k_wpT in ['C', 'Up', 'Down']:\n",
    "                    h_name = k\n",
    "                    if k_wpT != 'C':\n",
    "                        h_name += '__B0pT' + k_wpT\n",
    "                        \n",
    "                    w = cal_pT.f[k_wpT](d['MC_B_pt'][q2_bin])\n",
    "                    \n",
    "                    norm = np.sum(cal_pT.f[k_wpT](d['MC_B_pt']))\n",
    "                    h = create_TH1D(d[var][q2_bin], name=h_name, title=h_name, \n",
    "                                    binning=binning[var][i_q2], \n",
    "                                    opt='underflow,overflow',\n",
    "                                    weights=w,\n",
    "                                    scale_histo=1./norm,\n",
    "                                   )\n",
    "                    \n",
    "                    histo[cat_name][h_name] = h\n",
    "            elif k in ['Dstst']:\n",
    "                norm = float(d[var].shape[0])\n",
    "                h = create_TH1D(d[var][q2_bin], name=k, title=k, \n",
    "                                binning=binning[var][i_q2], \n",
    "                                opt='underflow,overflow',\n",
    "                                scale_histo=1./norm,\n",
    "                               )\n",
    "                histo[cat_name][k] = h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Pseudo-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_evts = {}\n",
    "for n in dSet.keys():\n",
    "    if n == 'data':\n",
    "        continue\n",
    "    print '--->', n\n",
    "    xsec_eff = dic_MCeff[n]['xsec']\n",
    "    print 'Pythia xsec: {:1.2e} pb'.format(xsec_eff)\n",
    "    xsec_eff *= dic_decayBR[n]\n",
    "    print 'Forced decays BR: {:1.2e}'.format(dic_decayBR[n])\n",
    "    xsec_eff *= dic_MCeff[n]['CMSSWFilterEff']\n",
    "    print'Eff CMSSW filter: {:1.2e}'.format(dic_MCeff[n]['CMSSWFilterEff'])\n",
    "    xsec_eff *= dic_MCeff[n]['ntupplizerEff']\n",
    "    print'Eff ntuplizer: {:1.2e}'.format(dic_MCeff[n]['ntupplizerEff'])\n",
    "    xsec_eff *= dic_MCeff[n]['analysisSelEff']\n",
    "    print'Eff selection: {:1.2e}'.format(dic_MCeff[n]['analysisSelEff'])\n",
    "    xsec_eff *= 1e3 # pb -> fb\n",
    "    print '\\nExpected evts/fb: {:.0f}'.format(xsec_eff)\n",
    "    print '\\n'\n",
    "    expected_evts[n] = xsec_eff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SM_RDst = 0.26\n",
    "rawR_exp = SM_RDst*expected_evts['tau']/expected_evts['mu']\n",
    "print 'Raw ratio: {:1.2e}'.format(rawR_exp)\n",
    "f_Hc = expected_evts['Hc']/np.sum(expected_evts.values())\n",
    "print 'f_Hc: {:1.2e}'.format(f_Hc)\n",
    "f_Dstst = expected_evts['Dstst']/np.sum(expected_evts.values())\n",
    "print 'f_Dstst: {:1.2e}'.format(f_Dstst)\n",
    "\n",
    "f_bkg = f_Hc + f_Dstst\n",
    "if fit_real_data:\n",
    "    from lumi_utilities import getLumiReport\n",
    "    lumi_tot, lumi_dic = getLumiReport(glob(file_loc['data']))\n",
    "else:    \n",
    "    lumi_tot = 40. #fb-1\n",
    "    if trgMu_pt_region == 'high':\n",
    "        N_exp_data = 98500\n",
    "    elif trgMu_pt_region == 'mid':\n",
    "        N_exp_data = 47000\n",
    "    N_exp = N_exp_data*lumi_tot/5.25\n",
    "    N_B2mu_inj = N_exp*(1-f_bkg)/(1+rawR_exp)\n",
    "\n",
    "    n_mu = np.random.poisson(lam=N_B2mu_inj)\n",
    "    w_mu = []\n",
    "    sum_w_mu = 0\n",
    "    idx_mu = []\n",
    "    print('Generating {:.1f}k pseudo-events of mu'.format(n_mu*1.0e-3))\n",
    "    pb = ProgressBar(n_mu)\n",
    "    while sum_w_mu < n_mu:\n",
    "        pb.show(np.ceil(sum_w_mu))\n",
    "        i = np.random.randint(0, dSet['mu']['q2'].shape[0], size=(1, ))[0]\n",
    "        w = cal_pT.f['C'](dSet['mu']['MC_B_pt'][i])\n",
    "        w *= dSet['mu']['wh_CLNCentral'][i]\n",
    "        idx_mu.append(i)\n",
    "        w_mu.append(w)\n",
    "        sum_w_mu += w\n",
    "    pb.show(n_mu-1)\n",
    "\n",
    "    n_tau = np.random.poisson(lam=N_B2mu_inj*rawR_exp)\n",
    "    w_tau = []\n",
    "    idx_tau = []\n",
    "    print('Generating {:.1f}k pseudo-events of tau'.format(n_tau*1.0e-3))\n",
    "    pb = ProgressBar(n_tau)\n",
    "    pb.show(0)\n",
    "    while np.sum(w_tau) < n_tau:\n",
    "        pb.show(np.sum(w_tau))\n",
    "        i = np.random.randint(0, dSet['tau']['q2'].shape[0], size=(1, ))[0]\n",
    "        w = cal_pT.f['C'](dSet['tau']['MC_B_pt'][i])\n",
    "        w *= dSet['tau']['wh_CLNCentral'][i]\n",
    "        idx_tau.append(i)\n",
    "        w_tau.append(w)\n",
    "    pb.show(n_tau-1)\n",
    "    \n",
    "    n_Hc = np.random.poisson(lam=N_exp*f_Hc)\n",
    "    w_Hc = []\n",
    "    idx_Hc = []\n",
    "    print('Generating {:.1f}k pseudo-events of Hc'.format(n_Hc*1.0e-3))\n",
    "    pb = ProgressBar(n_Hc)\n",
    "    while np.sum(w_Hc) < n_Hc:\n",
    "        pb.show(np.sum(w_Hc))\n",
    "        i = np.random.randint(0, dSet['Hc']['q2'].shape[0], size=(1, ))[0]\n",
    "        w = cal_pT.f['C'](dSet['Hc']['MC_B_pt'][i])\n",
    "        idx_Hc.append(i)\n",
    "        w_Hc.append(w)\n",
    "    pb.show(n_Hc-1)\n",
    "    \n",
    "    n_Dstst = np.random.poisson(lam=N_exp*f_Dstst)\n",
    "    print('Generating {:.1f}k pseudo-events of Dstst'.format(n_Dstst*1.0e-3))\n",
    "    idx_Dstst = np.random.randint(0, dSet['Dstst']['q2'].shape[0], size=(n_Dstst, ))\n",
    "\n",
    "    pseudo_data = {}\n",
    "    pseudo_w = np.array(w_mu + w_tau + w_Hc + [1.]*len(idx_Dstst))\n",
    "    for var in binning.keys():\n",
    "        pseudo_data[var] = np.concatenate((dSet['mu'][var][idx_mu], \n",
    "                                           dSet['tau'][var][idx_tau],\n",
    "                                           dSet['Hc'][var][idx_Hc],\n",
    "                                           dSet['Dstst'][var][idx_Dstst]\n",
    "                                          ))\n",
    "\n",
    "    for i_q2 in range(len(binning['q2'])-1):\n",
    "        q2_l = binning['q2'][i_q2]\n",
    "        q2_h = binning['q2'][i_q2 + 1]\n",
    "\n",
    "        for var in ['M2_miss', 'Est_mu']:\n",
    "            cat_name = var+'_q2bin'+str(i_q2)\n",
    "\n",
    "            sel = np.logical_and(pseudo_data['q2'] > q2_l, pseudo_data['q2'] < q2_h)\n",
    "            \n",
    "            h = create_TH1D(pseudo_data[var][sel], \n",
    "                            name='data_obs', title='data obs',\n",
    "                            binning=binning[var][i_q2],\n",
    "                            opt='underflow,overflow',\n",
    "                            weights=pseudo_w[sel]\n",
    "                           )\n",
    "            h.Sumw2(0)\n",
    "            for i in range(1, h.GetNbinsX()+1):\n",
    "                h.SetBinContent(i, round(h.GetBinContent(i)))\n",
    "            h.Sumw2()\n",
    "            histo[cat_name]['data'] = h\n",
    "\n",
    "\n",
    "    print 'r_toy = {:.2f}%'.format(100.*n_tau/n_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_data = 0\n",
    "N_mu = 0\n",
    "N_tau = 0\n",
    "N_Hc = 0\n",
    "N_Dstst = 0\n",
    "for k, h_dic in histo.iteritems():\n",
    "    if 'Est_mu' in k:\n",
    "        N_data += h_dic['data'].Integral()\n",
    "        N_mu += h_dic['mu'].Integral()\n",
    "        N_tau += h_dic['tau'].Integral()\n",
    "        N_Hc += h_dic['Hc'].Integral()\n",
    "        N_Dstst += h_dic['Dstst'].Integral()\n",
    "N_B2mu_exp = N_data*(1-f_bkg)/(1+rawR_exp)\n",
    "\n",
    "print 'Number of data events: {:.0f}'.format(N_data)\n",
    "print 'Number of B0 -> D*munu expected: {:.0f}'.format(N_B2mu_exp)\n",
    "print 'Total norm'\n",
    "print 'Mu: {:.3f}'.format(N_mu)\n",
    "print 'Tau: {:.3f}'.format(N_tau)\n",
    "print 'Hc: {:.3f}'.format(N_Hc)\n",
    "print 'Dstst: {:.3f}'.format(N_Dstst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histo_file_dir = '../data/_root/histos4side/'\n",
    "if not os.path.isdir(histo_file_dir):\n",
    "    os.makedirs(histo_file_dir)\n",
    "histo_file_loc = {}\n",
    "for cat_name, h_dic in histo.iteritems():\n",
    "    histo_file_loc[cat_name] = histo_file_dir+'{}_{}.root'.format(card_name, cat_name)\n",
    "    tf = rt.TFile(histo_file_loc[cat_name], 'recreate')\n",
    "    for v in h_dic.values():\n",
    "        v.Write()\n",
    "    tf.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CMS_lumi.integrated_lumi = lumi_tot\n",
    "scale_dic = {\n",
    "    'tau': N_B2mu_exp*rawR_exp, \n",
    "    'mu' : N_B2mu_exp,\n",
    "    'Hc' : N_data*f_Hc,\n",
    "    'Dstst' : N_data*f_Dstst\n",
    "            }\n",
    "\n",
    "c = plot_gridVarQ2(CMS_lumi, binning, histo, scale_dic, min_y=1, logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_processes = ['tau', 'mu']\n",
    "bkg_processes = ['Hc', 'Dstst']\n",
    "processes = sig_processes + bkg_processes\n",
    "categories = np.sort([k for k in histo.keys() if not '__' in k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T01:36:12.367638Z",
     "start_time": "2019-05-14T01:36:12.352769Z"
    }
   },
   "outputs": [],
   "source": [
    "card_location = 'cards/{}.txt'.format(card_name)\n",
    "fc = open(card_location, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T01:36:12.392125Z",
     "start_time": "2019-05-14T01:36:12.371738Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of different categories\n",
    "card = 'imax *\\n'\n",
    "# number of processes minus one\n",
    "card += 'jmax {}\\n'.format(len(processes)-1)\n",
    "# number of nuissance parameters\n",
    "card += 'kmax *\\n'\n",
    "card += '--------------------------------------------------------------\\n'\n",
    "\n",
    "# shape file location\n",
    "for k in categories:\n",
    "    card += 'shapes * {} {} $PROCESS $PROCESS__$SYSTEMATIC\\n'.format(k, histo_file_loc[k])\n",
    "card += '--------------------------------------------------------------\\n'\n",
    "\n",
    "# number of events observed\n",
    "card += 'bin ' + ' '.join(categories) + '\\n'\n",
    "obs = map(lambda k: '{:.0f}'.format(histo[k]['data'].Integral()), categories)\n",
    "obs = ' '.join(obs)\n",
    "card += 'observation ' + obs + '\\n'\n",
    "card += '--------------------------------------------------------------\\n'\n",
    "\n",
    "\n",
    "# MC expected events\n",
    "aux_bin = ''\n",
    "aux_proc_name = ''\n",
    "aux_proc_id = ''\n",
    "aux_proc_rate = ''\n",
    "for c, p in itertools.product(categories, processes):\n",
    "    aux_bin += ' '+c\n",
    "    aux_proc_name += ' '+p\n",
    "    aux_proc_id += ' '+str(np.argmax(np.array(processes) == p))\n",
    "    aux_proc_rate += ' {:.4f}'.format(histo[c][p].Integral())\n",
    "    \n",
    "card += 'bin' + aux_bin + '\\n'\n",
    "card += 'process' + aux_proc_name + '\\n'\n",
    "# Zero or negative for sig and positive for bkg\n",
    "card += 'process' + aux_proc_id + '\\n'\n",
    "# Expected rate\n",
    "card += 'rate' + aux_proc_rate + '\\n'\n",
    "card += '--------------------------------------------------------------\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add additional rate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card += 'N_B2mu rateParam * tau {:.1f} [0,{:.0f}]\\n'.format(scale_dic['mu'], 1.2*N_data)\n",
    "card += 'N_B2mu rateParam * mu {:.1f} [0,{:.0f}]\\n'.format(scale_dic['mu'], 1.2*N_data)\n",
    "card += 'N_B2DstHc rateParam * Hc {:.1f} [0,{:.0f}]\\n'.format(scale_dic['Hc'], 1.2*N_data)\n",
    "card += 'N_B2Dstst rateParam * Dstst {:.1f} [0,{:.0f}]\\n'.format(scale_dic['Dstst'], 1.2*N_data)\n",
    "card += '--------------------------------------------------------------\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_B0pT = ''\n",
    "for c, p in itertools.product(categories, processes):\n",
    "    if p in ['tau', 'mu', 'Hc']:\n",
    "        aux_B0pT += ' 1.'\n",
    "    else:\n",
    "        aux_B0pT += ' -'\n",
    "card += 'B0pT shape' + aux_B0pT + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_pFF in ['R0', 'R1', 'R2', 'RhoSq']:\n",
    "    aux = ''\n",
    "    for c, p in itertools.product(categories, processes):\n",
    "        if p in ['tau', 'mu']:\n",
    "            aux += ' 1.'\n",
    "        else:\n",
    "            aux += ' -'\n",
    "    card += 'B2DstCLN{} shape'.format(n_pFF) + aux + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC statistic systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card += '* autoMCStats 0 1 1\\n'\n",
    "card += '--------------------------------------------------------------\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining groups of systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoMCStats group = defined by default when using autoMCStats\n",
    "card += 'B2DstFF group = B2DstCLN' + ' B2DstCLN'.join(['R0', 'R1', 'R2', 'RhoSq']) + '\\n'\n",
    "card += 'bkgMC_norm group = N_B2DstHc N_B2Dstst\\n'\n",
    "card += 'all_card group = N_B2mu N_B2DstHc N_B2Dstst B0pT\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T01:36:12.412621Z",
     "start_time": "2019-05-14T01:36:12.396482Z"
    }
   },
   "outputs": [],
   "source": [
    "print card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T01:36:12.432835Z",
     "start_time": "2019-05-14T01:36:12.416805Z"
    }
   },
   "outputs": [],
   "source": [
    "fc.write(card)\n",
    "fc.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = 'results/' + card_name\n",
    "\n",
    "if os.path.isdir(outdir):\n",
    "    os.system('rm -rf ' + outdir)\n",
    "os.system('mkdir ' + outdir);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmd = 'text2workspace.py ' + card_location \n",
    "cmd += ' -o ' + card_location.replace('.txt', '.root')\n",
    "cmd += ' --no-b-only'\n",
    "cmd += ' --verbose 1'\n",
    "# cmd += ' --no-wrappers'\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Maximum Likelyhood fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'combine'\n",
    "cmd += ' -M FitDiagnostics'\n",
    "cmd += ' --robustFit 1'\n",
    "cmd += ' --cminDefaultMinimizerStrategy 0'\n",
    "cmd += ' --skipBOnlyFit'\n",
    "cmd += ' -d ' + card_location.replace('.txt', '.root')\n",
    "cmd += ' -D ' + histo[histo.keys()[0]]['data'].GetName()\n",
    "cmd += ' --X-rtd MINIMIZER_analytic'\n",
    "cmd += ' --setParameters r={:.2f}'.format(rawR_exp)\n",
    "cmd += ' --setParameterRanges r=0.001,1'\n",
    "cmd += ' --trackParameters N_B2mu,N_B2DstHc,N_B2Dstst'\n",
    "cmd += ' -n {}'.format(card_name)\n",
    "cmd += ' --out ' + outdir\n",
    "cmd += ' --saveShapes --saveWithUncertainties'\n",
    "cmd += ' --plots'\n",
    "cmd += ' --verbose 1'\n",
    "\n",
    "print cmd\n",
    "os.system(cmd)\n",
    "os.system('mv combine_logger.out ' + outdir + '/')\n",
    "os.system('mv ./*.root ' + outdir + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmd = 'python diffNuisances.py '.format(os.environ['CMSSW_BASE'])\n",
    "cmd += glob(outdir + '/fitDiagnostics{}.root'.format(card_name))[0]\n",
    "cmd += ' --skipFitB'\n",
    "cmd += ' -g {}/nuisance_pull.root'.format(outdir)\n",
    "print cmd\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = ur.open(glob(outdir + '/higgsCombine{}.FitDiagnostics.mH120.root'.format(card_name))[0])\n",
    "c, d, u, _ = f['limit']['limit'].array()\n",
    "print 'r = {:.1f} +{:.1f}/-{:.1f} %'.format(100*c, 100*(u-c), 100*(c-d))\n",
    "\n",
    "N_B2mu = f['limit']['trackedParam_N_B2mu'].array()[0]\n",
    "print 'N_B2mu = {:.0f}'.format(N_B2mu)\n",
    "N_B2DstHc = f['limit']['trackedParam_N_B2DstHc'].array()[0]\n",
    "print 'N_B2DstHc = {:.0f}'.format(N_B2DstHc)\n",
    "N_B2Dstst = f['limit']['trackedParam_N_B2Dstst'].array()[0]\n",
    "print 'N_B2Dstst = {:.0f}'.format(N_B2Dstst)\n",
    "\n",
    "# Get post-fit shapes\n",
    "f = rt.TFile(glob(outdir + '/fitDiagnostics{}.root'.format(card_name))[0], 'READ')\n",
    "fd = f.shapes_fit_s\n",
    "\n",
    "print '\\n'\n",
    "histo_postfit = {}\n",
    "for cat, h_dic in histo.iteritems():\n",
    "    histo_postfit[cat] = {}\n",
    "    for n, h in h_dic.iteritems():\n",
    "        if '__' in n:\n",
    "            continue\n",
    "        h_post = h.Clone(h.GetName() + '_postfit')\n",
    "        if 'data' in n:\n",
    "            h_fit = fd.Get(cat+'/total')\n",
    "            h_data = h.Clone(h.GetName() + '_data')\n",
    "            for i in range(1, h_post.GetNbinsX()+1):\n",
    "                h_post.SetBinContent(i, h_fit.GetBinContent(i))\n",
    "                h_post.SetBinError(i, h_fit.GetBinError(i))     \n",
    "            \n",
    "            histo_postfit[cat]['total'] = h_post\n",
    "            histo_postfit[cat][n] = h_data\n",
    "        else:\n",
    "            h_fit = fd.Get(cat+'/'+n)\n",
    "            for i in range(1, h_post.GetNbinsX()+1):\n",
    "                h_post.SetBinContent(i, h_fit.GetBinContent(i))\n",
    "                h_post.SetBinError(i, h_fit.GetBinError(i)) \n",
    "\n",
    "            histo_postfit[cat][n] = h_post\n",
    "            \n",
    "c_out = plot_gridVarQ2(CMS_lumi, binning, histo_postfit, scale_dic={}, draw_pulls=True, pulls_ylim=[0.7, 1.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_out.SaveAs('gridPlot_side.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run likelyhood scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUncertainty(name, verbose=True):\n",
    "    f = ur.open(name)\n",
    "    r_arr = f['limit']['r'].array()\n",
    "    nll_arr = f['limit']['deltaNLL'].array()\n",
    "    c = r_arr[0]\n",
    "    r_u = r_arr[r_arr>r_arr[0]]\n",
    "    nll_u = nll_arr[r_arr>r_arr[0]]\n",
    "    f_u = interp1d(nll_u, r_u, 'quadratic')\n",
    "    u = f_u(0.5)\n",
    "    r_l = r_arr[r_arr<r_arr[0]]\n",
    "    nll_l = nll_arr[r_arr<r_arr[0]]\n",
    "    f_l = interp1d(nll_l, r_l, 'quadratic')\n",
    "    l = f_l(0.5)\n",
    "    if verbose:\n",
    "        print '----------------------------------'\n",
    "        print 'r = {:.2f} +{:.2f}/-{:.2f} %'.format(100*c, 100*(u-c), 100*(c-l))\n",
    "        print 'Sigma = {:.3f}'.format((u-l)*0.5)\n",
    "        print '----------------------------------\\n'\n",
    "    return c, c-l, u-c, (u-l)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'combine'\n",
    "cmd += ' -M MultiDimFit'\n",
    "cmd += ' --algo grid --points=100'\n",
    "cmd += ' --robustFit 1'\n",
    "cmd += ' -d ' + card_location.replace('.txt', '.root')\n",
    "cmd += ' -D ' + histo[histo.keys()[0]]['data'].GetName()\n",
    "cmd += ' --X-rtd MINIMIZER_analytic'\n",
    "cmd += ' --rMin={:.4f} --rMax={:.4f}'.format(c - 5*(c-d), c + 5*(u-c))\n",
    "cmd += ' -n {}_nominal'.format(card_name)\n",
    "cmd += ' --verbose -1'\n",
    "print cmd\n",
    "os.system(cmd)\n",
    "cmd = 'plot1DScan.py higgsCombine{}_nominal.MultiDimFit.mH120.root'.format(card_name)\n",
    "cmd += '; mv scan.png scan_nominal.png'\n",
    "os.system(cmd)\n",
    "res_nominal = getUncertainty('higgsCombine{}_nominal.MultiDimFit.mH120.root'.format(card_name))\n",
    "display(Image(filename='scan_nominal.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainy breakdown by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'combine'\n",
    "cmd += ' -M MultiDimFit'\n",
    "cmd += ' --algo none'\n",
    "cmd += ' --cminDefaultMinimizerStrategy=2'\n",
    "cmd += ' --robustFit 1'\n",
    "cmd += ' -d ' + card_location.replace('.txt', '.root')\n",
    "cmd += ' -D ' + histo[histo.keys()[0]]['data'].GetName()\n",
    "cmd += ' --X-rtd MINIMIZER_analytic'\n",
    "cmd += ' --setParameters r=0.1'\n",
    "cmd += ' --rMin=0 --rMax=1'\n",
    "cmd += ' -n {}_bestfit'.format(card_name)\n",
    "cmd += ' --saveWorkspace'\n",
    "cmd += ' --verbose -1'\n",
    "print cmd\n",
    "os.system(cmd);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical uncertainty\n",
    "cmd = 'combine'\n",
    "cmd += ' -M MultiDimFit'\n",
    "cmd += ' --algo grid --points=100'\n",
    "cmd += ' --cminDefaultMinimizerStrategy=2'\n",
    "cmd += ' --robustFit 1'\n",
    "cmd += ' -d higgsCombine{}_bestfit.MultiDimFit.mH120.root'.format(card_name)\n",
    "cmd += ' -D ' + histo[histo.keys()[0]]['data'].GetName()\n",
    "cmd += ' --X-rtd MINIMIZER_analytic'\n",
    "cmd += ' --rMin={:.4f} --rMax={:.4f}'.format(c - 5*(c-d), c + 5*(u-c))\n",
    "cmd += ' -n {}_stat'.format(card_name)\n",
    "cmd += ' --snapshotName MultiDimFit'\n",
    "cmd += ' --freezeParameters allConstrainedNuisances'\n",
    "cmd += ' --verbose -1'\n",
    "print cmd\n",
    "os.system(cmd);\n",
    "res_stat = getUncertainty('higgsCombine{}_stat.MultiDimFit.mH120.root'.format(card_name))\n",
    "cmd = 'plot1DScan.py higgsCombine{}_nominal.MultiDimFit.mH120.root'.format(card_name)\n",
    "cmd += ' --others \"higgsCombine{}_stat.MultiDimFit.mH120.root:Freeze all:2\"'.format(card_name)\n",
    "cmd += ' --breakdown syst,stat'\n",
    "cmd += '; mv scan.png scan_stat.png'\n",
    "print cmd\n",
    "os.system(cmd)\n",
    "display(Image(filename='scan_stat.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC Statistics\n",
    "cmd = 'combine'\n",
    "cmd += ' -M MultiDimFit'\n",
    "cmd += ' --algo grid --points=100'\n",
    "cmd += ' --cminDefaultMinimizerStrategy=2'\n",
    "cmd += ' --robustFit 1'\n",
    "cmd += ' -d higgsCombine{}_bestfit.MultiDimFit.mH120.root'.format(card_name)\n",
    "cmd += ' -D ' + histo[histo.keys()[0]]['data'].GetName()\n",
    "cmd += ' --X-rtd MINIMIZER_analytic'\n",
    "cmd += ' --rMin={:.4f} --rMax={:.4f}'.format(c - 5*(c-d), c + 5*(u-c))\n",
    "cmd += ' -n {}_MCstat'.format(card_name)\n",
    "cmd += ' --snapshotName MultiDimFit'\n",
    "cmd += ' --freezeNuisanceGroups=autoMCStats'\n",
    "cmd += ' --verbose -1'\n",
    "print cmd\n",
    "os.system(cmd);\n",
    "res_MCstat = getUncertainty('higgsCombine{}_MCstat.MultiDimFit.mH120.root'.format(card_name))\n",
    "cmd = 'plot1DScan.py higgsCombine{}_nominal.MultiDimFit.mH120.root'.format(card_name)\n",
    "cmd += ' --others'\n",
    "cmd += ' \"higgsCombine{}_MCstat.MultiDimFit.mH120.root:Freeze MC stat:4\"'.format(card_name)\n",
    "cmd += ' \"higgsCombine{}_stat.MultiDimFit.mH120.root:Freeze all:2\"'.format(card_name)\n",
    "cmd += ' --breakdown MCstat,syst,stat'\n",
    "cmd += '; mv scan.png scan_MCstat.png'\n",
    "print cmd\n",
    "os.system(cmd)\n",
    "display(Image(filename='scan_MCstat.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('mv higgsCombine*.root ' + outdir + '/')\n",
    "os.system('mv scan* ' + outdir + '/')\n",
    "os.system('mv combine_logger.out ' + outdir + '/');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pull/impact plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit first the POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmd = 'combineTool.py -M Impacts --doInitialFit -m 120'\n",
    "cmd += ' --robustFit 1'\n",
    "cmd += ' -d ' + card_location.replace('.txt', '.root')\n",
    "cmd += ' -D ' + histo[histo.keys()[0]]['data'].GetName()\n",
    "cmd += ' --X-rtd MINIMIZER_analytic'\n",
    "cmd += ' --setParameters r={:.2f}'.format(rawR_exp)\n",
    "cmd += ' --setParameterRanges r=0.001,1'\n",
    "cmd += ' -n {}'.format(card_name)\n",
    "cmd += ' --out ' + outdir\n",
    "cmd += ' --verbose 0'\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Perform a similar scan for each nuisance parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'combineTool.py -M Impacts --doFits -m 120'\n",
    "cmd += ' --robustFit 1'\n",
    "cmd += ' --parallel 8'\n",
    "cmd += ' -d ' + card_location.replace('.txt', '.root')\n",
    "cmd += ' -D ' + histo[histo.keys()[0]]['data'].GetName()\n",
    "cmd += ' --X-rtd MINIMIZER_analytic'\n",
    "# cmd += ' --setParameters r={:.2f}'.format(rawR_exp)\n",
    "# cmd += ' --setParameterRanges r=0.001,1'\n",
    "cmd += ' -n {}'.format(card_name)\n",
    "cmd += ' --out ' + outdir\n",
    "cmd += ' --verbose 0'\n",
    "print cmd\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'combineTool.py -M Impacts -o impacts.json -m 120'\n",
    "cmd += ' -d ' + card_location.replace('.txt', '.root')\n",
    "cmd += ' -n {}'.format(card_name)\n",
    "print cmd\n",
    "os.system(cmd);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename = {\n",
    "'B0pT': 'B_{0} p_{T} spectrum',\n",
    "'N_B2mu':'N_{B#rightarrow D*#mu#nu}',\n",
    "'N_B2DstHc':'N_{B#rightarrow D*H_{c}}',\n",
    "'N_B2Dstst':'N_{B#rightarrow D**#mu#nu}',\n",
    "'B2DstCLNR0':'R_{0} (CLN B#rightarrow D*)',\n",
    "'B2DstCLNR1':'R_{1} (CLN B#rightarrow D*)',\n",
    "'B2DstCLNR2':'R_{2} (CLN B#rightarrow D*)',\n",
    "'B2DstCLNRhoSq':'#rho^{2} (CLN B#rightarrow D*)'\n",
    "}\n",
    "json.dump(rename, open('rename.json', 'w'))\n",
    "\n",
    "cmd = 'plotImpacts.py -i impacts.json -o impacts -t rename.json'\n",
    "os.system(cmd)\n",
    "IFrame(\"impacts.pdf\", width=900, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('mv *.root {}/'.format(outdir))\n",
    "os.system('mv impacts.* {}/'.format(outdir))\n",
    "os.system('mv rename.json {}/'.format(outdir))\n",
    "os.system('mv combine_logger.out {}/'.format(outdir));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Goodness of fit test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the observed test stat value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'combine'\n",
    "cmd += ' -M GoodnessOfFit'\n",
    "cmd += ' -d ' + card_location\n",
    "cmd += ' -D ' + histo[histo.keys()[0]]['data'].GetName()\n",
    "cmd += ' -n Obs'                                    # Just the output name\n",
    "cmd += ' -t 0'                                      # Don't run toys\n",
    "cmd += ' -s -1'                                     # Random seed\n",
    "cmd += ' --toysNoSystematics --algo=saturated'\n",
    "# cmd += ' --toysFrequentist'\n",
    "cmd += ' --X-rtd MINIMIZER_analytic'\n",
    "cmd += ' --setParameters r=0.1'\n",
    "cmd += ' --setParameterRanges r=0.001,1'\n",
    "cmd += ' --trackParameters N_B2mu'\n",
    "cmd += ' --plots'\n",
    "cmd += ' --verbose 9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print cmd\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the test stat toy distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'combine'\n",
    "cmd += ' -M GoodnessOfFit'\n",
    "cmd += ' -d ' + card_location\n",
    "cmd += ' -D ' + histo[histo.keys()[0]]['data'].GetName()\n",
    "cmd += ' -n Toys'                                   # Just the output name\n",
    "cmd += ' -t 300'                                    # Number of toys to run\n",
    "cmd += ' -s -1'                                     # Random seed\n",
    "cmd += ' --toysNoSystematics --algo=saturated'\n",
    "# cmd += ' --toysFrequentist'\n",
    "# cmd += '--expectSignal=0'                           # Depending on the hypothesis to test. If none, r is fluctruated\n",
    "cmd += ' --X-rtd MINIMIZER_analytic'\n",
    "cmd += ' --setParameters r=0.1'\n",
    "cmd += ' --setParameterRanges r=0.001,1'\n",
    "cmd += ' --trackParameters N_B2mu'\n",
    "cmd += ' --plots'\n",
    "cmd += ' --verbose 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print cmd\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine them to get the p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_obs = glob('higgsCombineObs.GoodnessOfFit.*.root')[0]\n",
    "# name_obs = glob(outdir+'/higgsCombineObs.GoodnessOfFit.*.root')[0]\n",
    "name_toys = glob('higgsCombineToys.GoodnessOfFit.*.root')[0]\n",
    "# name_toys = glob(outdir+'/higgsCombineToys.GoodnessOfFit.*.root')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ur.open(name_obs)\n",
    "s_obs = f['limit']['limit'].array()[0]\n",
    "\n",
    "f = ur.open(name_toys)\n",
    "s_toys = f['limit']['limit'].array()\n",
    "\n",
    "content, center, _ = plt.hist(s_toys, label='Toys')\n",
    "plt.plot([s_obs, s_obs], [0, np.max(content)], 'm--', label='Observed')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "p_val = np.sum(s_toys > s_obs)/float(s_toys.shape[0])\n",
    "print 'p-value: {:.1f}%'.format(100*p_val)\n",
    "if p_val < 0.01: print p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmd = 'combineTool.py'\n",
    "# cmd += ' -M CollectGoodnessOfFit'\n",
    "# cmd += ' --mass 120'\n",
    "# cmd += ' -o gof.json'\n",
    "# cmd += ' --input {} {}'.format(name_obs, name_toys)\n",
    "# print cmd\n",
    "# os.system(cmd)\n",
    "# os.system('plotGof.py gof.json -o gof --mass 120.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('mv *.root {}/'.format(outdir))\n",
    "os.system('mv *.dot {}/'.format(outdir))\n",
    "os.system('mv *.out {}/'.format(outdir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
