{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to collect the information of the generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T23:00:57.326302Z",
     "start_time": "2019-11-04T23:00:57.319852Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os, re, yaml, pickle\n",
    "import commands\n",
    "from glob import glob\n",
    "sys.path.append('../lib')\n",
    "\n",
    "import time, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T23:00:57.880990Z",
     "start_time": "2019-11-04T23:00:57.535016Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "import humanfriendly\n",
    "from progressBar import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T23:00:59.175644Z",
     "start_time": "2019-11-04T23:00:57.885084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.12/07\n"
     ]
    }
   ],
   "source": [
    "import uproot as ur\n",
    "import ROOT as rt\n",
    "rt.gErrorIgnoreLevel = rt.kError\n",
    "rt.RooMsgService.instance().setGlobalKillBelow(rt.RooFit.ERROR)\n",
    "import root_numpy as rtnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dic_dir = '/storage/user/ocerri/work/CMSSW_10_2_3/src/ntuplizer/BPH_RDntuplizer/production/samples.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T23:00:59.195419Z",
     "start_time": "2019-11-04T23:00:59.179871Z"
    }
   },
   "outputs": [],
   "source": [
    "class Bauble: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T23:01:19.700694Z",
     "start_time": "2019-11-04T23:01:19.684231Z"
    }
   },
   "outputs": [],
   "source": [
    "setName = 'SignalRegion'\n",
    "outDic = {}\n",
    "\n",
    "outDic['mu'] = Bauble()\n",
    "outDic['mu'].sample = 'B0_MuNuDmst_PU20'\n",
    "\n",
    "outDic['tau'] = Bauble()\n",
    "outDic['tau'].sample = 'B0_TauNuDmst_PU20'\n",
    "\n",
    "outDic['Hc'] = Bauble()\n",
    "outDic['Hc'].sample = 'B0_DmstHc_PU20'\n",
    "\n",
    "outDic['Dstst'] = Bauble()\n",
    "outDic['Dstst'].sample = 'Bp_MuNuDstst_PU20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T23:01:19.700694Z",
     "start_time": "2019-11-04T23:01:19.684231Z"
    }
   },
   "outputs": [],
   "source": [
    "# setName = 'SidePT'\n",
    "# outDic = {}\n",
    "\n",
    "# outDic['JPsiKst'] = Bauble()\n",
    "# outDic['JPsiKst'].sample = 'B0_JpsiKst_PU0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_loc_conf = '/mnt/hadoop/store/user/ocerri'\n",
    "ntuples_loc = '/storage/user/ocerri/BPhysics/data/cmsMC_private/{}/ntuples_B2*'\n",
    "samples = yaml.load(open(sample_dic_dir))\n",
    "\n",
    "for d in outDic.values():\n",
    "    d.MINIAOD_dirs = []\n",
    "    for directory in samples['samples'][d.sample]['parts']:\n",
    "        aux = glob(site_loc_conf + directory[:-38].replace('ocerri-','') + '/*/*')\n",
    "        d.MINIAOD_dirs += aux\n",
    "    \n",
    "    full_name = samples['samples'][d.sample]['dataset']\n",
    "    d.ntuples_dir = glob(ntuples_loc.format(full_name))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T23:01:20.322756Z",
     "start_time": "2019-11-04T23:01:20.307921Z"
    }
   },
   "outputs": [],
   "source": [
    "def getEff(k,N):\n",
    "    e = k/float(N)\n",
    "    de = np.sqrt(e*(1-e)/N)\n",
    "    return [e, de]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hadoop/store/user/ocerri/cmsMC_private_PU20_10-2-3/BPH_Tag-B0_MuNuDmst-pD0bar-kp_13TeV-pythia8_Hardbbbar_PTFilter5_0p0-evtgen_ISGW2_200117/200118_024956/0010/out_MINIAODSIM_10000.root\n"
     ]
    }
   ],
   "source": [
    "fn = glob(outDic['mu'].MINIAOD_dirs[-1]+'/*.root')[0]\n",
    "print fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load FWLite C++ libraries\n",
    "rt.gSystem.Load(\"libFWCoreFWLite.so\");\n",
    "rt.gSystem.Load(\"libDataFormatsFWLite.so\");\n",
    "rt.FWLiteEnabler.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load FWlite python libraries\n",
    "from DataFormats.FWLite import Lumis\n",
    "from DataFormats.FWLite import Handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1099\n",
      "150000\n",
      "0.00732666666667\n",
      "0.000220196679088\n"
     ]
    }
   ],
   "source": [
    "handle = {}\n",
    "handle['genInfo'] = [Handle('GenFilterInfo'), ('genFilterEfficiencyProducer', '', 'SIM')]\n",
    "\n",
    "for lumi in Lumis(fn):\n",
    "    prods = {}\n",
    "    for k,v in handle.iteritems():\n",
    "        lumi.getByLabel(v[1], v[0])\n",
    "        prods[k] = v[0].product()\n",
    "    print prods['genInfo'].numEventsPassed()\n",
    "    print prods['genInfo'].numEventsTotal()\n",
    "    print prods['genInfo'].filterEfficiency()\n",
    "    print prods['genInfo'].filterEfficiencyError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T23:02:17.419301Z",
     "start_time": "2019-11-04T23:02:09.765091Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "N_max = 10000\n",
    "\n",
    "for name, d in outDic.iteritems():\n",
    "    print '\\n--> ' + name\n",
    "    dicOut = {}\n",
    "    N_gen = 0\n",
    "    N_cuts = 0\n",
    "    xsec = []\n",
    "    xsec_err = []\n",
    "    \n",
    "    print 'Fetching logs files location'\n",
    "    step1log_list = []\n",
    "    for directory in d.MINIAOD_dirs:\n",
    "        step1log_list += glob(directory + '/step1log_*.root')\n",
    "    if N_max > 0 and N_max < len(step1log_list):\n",
    "        step1log_list = np.random.choice(step1log_list, N_max)\n",
    "    \n",
    "    print 'Analizing', len(step1log_list), 'logs'\n",
    "    pb = ProgressBar(maxEntry=len(step1log_list))\n",
    "    for i_j, step1log_file in enumerate(step1log_list):\n",
    "#         print step1log_file\n",
    "        pb.show(i_j)\n",
    "        cmd = 'tail -n 40 ' + step1log_file\n",
    "        status, step1_log_lines = commands.getstatusoutput(cmd)\n",
    "        for il, line in enumerate(step1_log_lines.split('\\n')):\n",
    "            if line.startswith('Before Filter: total cross section'):\n",
    "                aux = line[37:-3].split(' +- ')\n",
    "                xsec.append(float(aux[0]))\n",
    "                xsec_err.append(float(aux[1]))\n",
    "            elif line.startswith('Filter efficiency (event-level)='):\n",
    "                l=line[34:-24]\n",
    "                l = l.split(') = ')[0]\n",
    "                l = l.split(') / (')\n",
    "                N_cuts += int(l[0])\n",
    "                N_gen += int(l[1])\n",
    "                break\n",
    "        e, de = getEff(N_cuts, N_gen)\n",
    "    \n",
    "    xsec = np.array(xsec)\n",
    "    xsec_err = np.array(xsec_err)\n",
    "    s2 = np.square(xsec_err)\n",
    "    num = np.sum(xsec/s2)\n",
    "    den = np.sum(1./s2)\n",
    "    xsec = 1e3*num/den\n",
    "    xsec_err = 1e3*np.sqrt(1/den)\n",
    "    print 'Xsec: {:1.4e} +/- {:1.4e} fb ({:1.1e})'.format(xsec, xsec_err, xsec_err/xsec)\n",
    "    dicOut['xsec'] = [xsec, xsec_err]\n",
    "    \n",
    "    e, de = getEff(N_cuts, N_gen)\n",
    "    print 'CMSSW eff: {:1.3e} +/- {:1.3e} ({:1.1e})'.format(e,de, de/e)\n",
    "    dicOut['CMSSWFilterEff'] = [e, de]\n",
    "    continue\n",
    "    \n",
    "    cand_out_list = glob(d.ntuples_dir + '/out/job*.out')\n",
    "    N_analyzed = []\n",
    "    N_trg = []\n",
    "    N_cand = []\n",
    "    eff = []\n",
    "    for cand_out in cand_out_list:\n",
    "        step5_log_lines = open(cand_out).readlines()\n",
    "        eff_ln = []\n",
    "        for line in reversed(step5_log_lines):\n",
    "            if 'efficiency:' in line:\n",
    "                eff_ln.append(line)\n",
    "\n",
    "        aux = re.search('[0-9]+/[0-9]+', eff_ln[1]).group(0)\n",
    "        aux = aux.split('/')\n",
    "        N_analyzed.append(int(aux[1]))\n",
    "        N_trg.append(int(aux[0]))\n",
    "        \n",
    "        aux = re.search(': [0-9]+/', eff_ln[0]).group(0)\n",
    "        N_cand.append(int(aux[2:-1]))\n",
    "    \n",
    "    d.N_analyzed = np.array(N_analyzed)\n",
    "    d.N_trg = np.array(N_trg)\n",
    "#     print 'Avg N_trg:', float(np.sum(N_trg) / d.N_acc.shape[0])\n",
    "    d.N_cand = np.array(N_cand)\n",
    "#     print 'Avg N_cand:', float(np.sum(N_cand) / d.N_acc.shape[0])\n",
    "    d.ntuplizerEff = d.N_cand.astype(np.float)/d.N_analyzed\n",
    "    print 'Avg ntuplizer eff: {:1.2e}'.format(np.mean(d.ntuplizerEff))\n",
    "    \n",
    "    \n",
    "    ## Sanity checks\n",
    "    print 'Tot candidates:', np.sum(d.N_cand)\n",
    "#     n1 = np.sum(d.N_analyzed)\n",
    "#     n2 = np.sum(d.N_acc)\n",
    "#     if n1 != n2:\n",
    "#         print  np.sum(d.N_analyzed), np.sum(d.N_acc)\n",
    "#         print n1-n2\n",
    "\n",
    "    ## Save infos\n",
    "    dump_d = {}\n",
    "    dump_d['xsec'] = np.mean(d.xsec)\n",
    "    dump_d['CMSSWFilterEff'] = np.mean(d.CMSSWFilterEff)\n",
    "    dump_d['ntupplizerEff'] = np.mean(d.ntuplizerEff)   \n",
    "    dump_d['TotCand'] = np.sum(d.N_cand)\n",
    "    d.dump_d = dump_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_dic = {}\n",
    "for n, d in outDic.iteritems():\n",
    "    dump_dic[n] = d.dump_d\n",
    "pickle.dump(dump_dic, open('../data/'+setName+'MC_efficiencies_PU20.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, d in outDic.iteritems():\n",
    "    print name\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(12,4))\n",
    "    ax[0].hist(d.N_gen_cuts)\n",
    "    ax[0].set_xlabel('Events after gen cuts')\n",
    "    ax[1].hist(d.N_acc)\n",
    "    ax[1].set_xlabel('Events accepted')\n",
    "    ax[2].hist(d.N_cand)\n",
    "    ax[2].set_xlabel('Candidates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T23:02:19.878043Z",
     "start_time": "2019-11-04T23:02:19.872946Z"
    }
   },
   "outputs": [],
   "source": [
    "def bashDate2Unix(ln, debug=False):\n",
    "    template = '%c'\n",
    "    ln = ln.replace('CEST ', '')\n",
    "    ln = ln.replace('UTC ', '')\n",
    "    if debug:\n",
    "        print ln\n",
    "    else:\n",
    "        t = time.mktime(datetime.datetime.strptime(ln, template).timetuple())\n",
    "        return float(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T23:02:37.880908Z",
     "start_time": "2019-11-04T23:02:20.380651Z"
    }
   },
   "outputs": [],
   "source": [
    "for name in outDic.keys():\n",
    "    print '\\n--> ' + name\n",
    "    master_dir = outDic[name].dir + '/'\n",
    "    d = outDic[name]\n",
    "    d.driver_time = []\n",
    "    d.run_time =[]\n",
    "    d.tot_time =[]\n",
    "    \n",
    "    job_dir_list = glob(master_dir + 'jobs_out/out_*[0-9]')\n",
    "    pb = ProgressBar(maxEntry=len(job_dir_list))\n",
    "    for i_j, job_dir in enumerate(job_dir_list):\n",
    "        pb.show(i_j)\n",
    "        aux = os.path.basename(job_dir)\n",
    "        n_job = int(aux[4:])\n",
    "        \n",
    "        fn = glob(master_dir + 'jobs_out/out/job_{}_*.out'.format(n_job-1))[0]\n",
    "        if not os.path.isfile(fn): continue\n",
    "        test_log_lines = open(fn).readlines()\n",
    "        \n",
    "        try:\n",
    "            crono = [[],[],[],[],[]]\n",
    "            for i, ln in enumerate(test_log_lines):\n",
    "                for j in range(1,5):\n",
    "                    if 'Step '+str(j) in ln or 'Running step '+str(j) in ln:\n",
    "                        try:\n",
    "                            crono[j-1].append(bashDate2Unix(test_log_lines[i+1][:-1]))\n",
    "                        except:\n",
    "                            print '\\nError in job', n_job\n",
    "                            crono[j-1].append(crono[j-1][-1])\n",
    "            crono[4].append(bashDate2Unix(test_log_lines[-1][:-1]))\n",
    "\n",
    "            driver_time = []\n",
    "            run_time = []\n",
    "            for i in range(4):\n",
    "                driver_time.append(crono[i][1] - crono[i][0])\n",
    "                run_time.append(crono[i+1][0] - crono[i][1])\n",
    "            tot_time = crono[4][0] - crono[0][0]\n",
    "\n",
    "            d.driver_time.append(driver_time)\n",
    "            d.run_time.append(run_time)\n",
    "            d.tot_time.append(tot_time)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    d.driver_time = np.array(d.driver_time)\n",
    "    d.run_time = np.array(d.run_time)\n",
    "    d.tot_time = np.array(d.tot_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T23:02:38.054400Z",
     "start_time": "2019-11-04T23:02:37.885162Z"
    }
   },
   "outputs": [],
   "source": [
    "bins = plt.hist(d.tot_time/3600)\n",
    "plt.xlabel('Total time [h]')\n",
    "x = np.percentile(d.tot_time, 90)/3600\n",
    "plt.plot([x,x], [0,np.max(bins[0])], 'm--', lw=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T23:02:38.109083Z",
     "start_time": "2019-11-04T23:02:38.057315Z"
    }
   },
   "outputs": [],
   "source": [
    "for name in outDic.keys():\n",
    "    print '\\n--> ' + name\n",
    "    master_dir = outDic[name].dir + '/'\n",
    "    d = outDic[name]\n",
    "    d.MINIAOD_size = []\n",
    "    \n",
    "    job_dir_list = glob(master_dir + 'jobs_out/out_*[0-9]')\n",
    "    pb = ProgressBar(maxEntry=len(job_dir_list))\n",
    "    for i_j, job_dir in enumerate(job_dir_list):\n",
    "        pb.show(i_j)\n",
    "        aux = os.path.basename(job_dir)\n",
    "        n_job = int(aux[4:])\n",
    "        if not os.path.isfile(job_dir + '/step1.log'): continue\n",
    "        fname_MINI = master_dir + 'jobs_out/out_MINIAODSIM_{}.root'.format(n_job)\n",
    "        if not os.path.isfile(fname_MINI): continue\n",
    "        d.MINIAOD_size.append(os.path.getsize(fname_MINI))\n",
    "    \n",
    "    d.MINIAOD_size = np.array(d.MINIAOD_size)\n",
    "    print 'Avg MINIAOD size:', humanfriendly.format_size(np.mean(d.MINIAOD_size), binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T23:02:38.200211Z",
     "start_time": "2019-11-04T23:02:38.113666Z"
    }
   },
   "outputs": [],
   "source": [
    "for n, d in outDic.iteritems():\n",
    "    table = PrettyTable()\n",
    "    table.field_names = ['Step', 'Sub', 'Eff [%]', 'Driver time [s]', 'Running time/evt [s]']\n",
    "    \n",
    "    e, de = getEff(np.sum(d.N_acc), np.sum(d.N_gen_cuts))\n",
    "    dt = np.mean(d.driver_time[:, 0])\n",
    "    dr_ev = np.mean(d.run_time[:, 0]/d.N_acc)\n",
    "    table.add_row(['GEN-SIM', '-', '{:.2f} +/- {:.2f}'.format(100*e, 100*de), '{:.1f}'.format(dt), '{:.1f}'.format(dr_ev)])\n",
    "    \n",
    "    e, de = getEff(np.sum(d.N_gen_cuts), np.sum(d.N_gen))\n",
    "    table.add_row(['', 'Pythia Gen', 'x {:.1f}'.format(1./e), '-', '-'])\n",
    "    table.add_row(['', 'Gen Filter', '100.0', '-', '-'])\n",
    "    e, de = getEff(np.sum(d.N_acc), np.sum(d.N_gen_cuts))\n",
    "    table.add_row(['', 'CMSSW Filter', '{:.2f} +/- {:.2f}'.format(100*e, 100*de), '-', '-'])\n",
    "\n",
    "    table.add_row(['RAW', '-', '100.0', '{:.1f}'.format(np.mean(d.driver_time[:,1])), '{:.1f}'.format(np.mean(d.run_time[:,1]/d.N_acc))])\n",
    "    table.add_row(['AOD', '-', '100.0', '{:.1f}'.format(np.mean(d.driver_time[:,2])), '{:.1f}'.format(np.mean(d.run_time[:,2]/d.N_acc))])\n",
    "    table.add_row(['MINIAOD', '-', '100.0', '{:.1f}'.format(np.mean(d.driver_time[:,3])), '{:.1f}'.format(np.mean(d.run_time[:,3]/d.N_acc))])\n",
    "    \n",
    "    e, de = getEff(np.sum(d.N_cand), np.sum(d.N_acc))\n",
    "    table.add_row(['CAND', '-', '{:.2f} +/- {:.2f}'.format(100*e, 100*de), '-', '-'])\n",
    "    e, de = getEff(np.sum(d.N_trg), np.sum(d.N_acc))\n",
    "    table.add_row(['', 'BPH Trg', '{:.2f} +/- {:.2f}'.format(100*e, 100*de), '-', '-'])\n",
    "    e, de = getEff(np.sum(d.N_cand), np.sum(d.N_trg))\n",
    "    table.add_row(['', 'Cand. sel.', '{:.2f} +/- {:.2f}'.format(100*e, 100*de), '-', '-'])\n",
    "    \n",
    "    table.add_row(len(table.field_names)*[''])\n",
    "    \n",
    "    e, de = getEff(np.sum(d.N_cand), np.sum(d.N_gen_cuts))\n",
    "    table.add_row(['Tot', '', '{:.3f} +/- {:.3f}'.format(100*e, 100*de), '{:.1f}'.format(np.mean(np.sum(d.driver_time, axis=1))), '{:.1f}(*)'.format(np.mean(np.sum(d.run_time, axis=1)/d.N_acc))])\n",
    "\n",
    "    print n\n",
    "    print table.get_string()\n",
    "    \n",
    "    tabsum = PrettyTable()\n",
    "    tabsum.field_names = ['Evts req.', '# MINIAOD Evts', '# Cand.', 'Tot. Time [h]']\n",
    "    \n",
    "    MINIAOD_size = humanfriendly.format_size(np.mean(d.MINIAOD_size), binary=True)\n",
    "    tabsum.add_row(['{:.0f}k'.format(1e-3*np.mean(d.N_gen_cuts)), '{:.0f} ({})'.format(np.mean(d.N_acc), MINIAOD_size), '{:.0f}'.format(np.sum(d.N_cand) / d.N_acc.shape[0]), '{:.1f}'.format(np.mean(d.tot_time)/3600.)])\n",
    "    print tabsum.get_string()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
