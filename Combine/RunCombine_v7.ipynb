{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New:\n",
    " - Additional D* with multiple pions sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_name = 'v7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_real_data = False\n",
    "category = 'low'\n",
    "SM_RDst = 0.26\n",
    "fastRun = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, pickle\n",
    "from glob import glob\n",
    "sys.path.append('../lib')\n",
    "sys.path.append('../analysis')\n",
    "from categoriesDef import categories\n",
    "import itertools\n",
    "import json, yaml\n",
    "from IPython.display import IFrame, Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T01:36:12.149848Z",
     "start_time": "2019-05-14T01:36:11.232339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.12/07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "from array import array\n",
    "\n",
    "import uproot as ur\n",
    "import ROOT as rt\n",
    "rt.gErrorIgnoreLevel = rt.kError\n",
    "rt.RooMsgService.instance().setGlobalKillBelow(rt.RooFit.ERROR)\n",
    "import root_numpy as rtnp\n",
    "\n",
    "from analysis_utilities import drawOnCMSCanvas, getEff, DSetLoader\n",
    "from pT_calibration_reader import pTCalReader\n",
    "from histo_utilities import create_TH1D, create_TH2D, std_color_list\n",
    "from gridVarQ2Plot import plot_gridVarQ2, plot_SingleAddTkMassHad, plot_SingleAddTkMassVis\n",
    "from progressBar import ProgressBar\n",
    "from lumi_utilities import getLumiByTrigger\n",
    "\n",
    "import CMS_lumi, tdrstyle\n",
    "tdrstyle.setTDRStyle()\n",
    "CMS_lumi.writeExtraText = 1\n",
    "\n",
    "if fit_real_data:\n",
    "    CMS_lumi.extraText = \"     Preliminary\"\n",
    "else:\n",
    "    CMS_lumi.extraText = \"     Simulation Preliminary\"\n",
    "\n",
    "donotdelete = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = categories[category]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "binning = {\n",
    "    'M2_miss' : [\n",
    "        array('d', [-2.5] + list(np.arange(-1.8, 0.8, 0.4)) + [8] ),\n",
    "        array('d', [-2.5] + list(np.arange(-1.8, 3.0, 0.4)) + [8] ),\n",
    "        array('d', [-2.5] + list(np.arange(-1.8, 5.2, 0.4)) + [8] ),\n",
    "        array('d', [-2.5] + list(np.arange(-1.8, 7.6, 0.4)) + [8] ),\n",
    "    ],\n",
    "    'Est_mu'  : [\n",
    "        array('d', [0.3] + list(np.arange(0.8, 2.3, 0.1)) + [2.5] ),\n",
    "        array('d', [0.3] + list(np.arange(0.8, 2.5, 0.1)) + [2.5] ),\n",
    "        [20, 0.3, 2.500],\n",
    "        [20, 0.3, 2.500],\n",
    "    ],\n",
    "    'tkMassHad_0' : [30, 2.13, 2.73],\n",
    "    'tkMassVis_0' : [25, 2.7, 5.3]\n",
    "}\n",
    "if fit_real_data:\n",
    "    binning['q2'] = array('d', [-2, 2.5, 6, 9.4, 12])\n",
    "else:\n",
    "    binning['q2'] = array('d', [-2, 2.5, 6, 9.4, 12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#They all have to be produced with the same pileup\n",
    "MCsample = {\n",
    "'mu' : DSetLoader('B0_MuNuDmst_PUc0'),\n",
    "'tau' : DSetLoader('B0_TauNuDmst_PUc0'),\n",
    "'Hc' : DSetLoader('B0_DmstHc_PUc0'),\n",
    "'DstPip' : DSetLoader('Bp_MuNuDstst_Pip_PUc0'),\n",
    "'DstPi0' : DSetLoader('B0_DmstPi0MuNu_PUc0'),\n",
    "'DstPipPi0' : DSetLoader('Bp_MuNuDstst_PipPi0_PUc0'),\n",
    "'DstPipPim' : DSetLoader('B0_MuNuDstst_PipPim_PUc0'),\n",
    "'DstPi0Pi0' : DSetLoader('B0_MuNuDstst_Pi0Pi0_PUc0'),\n",
    "}\n",
    "dSet = {}\n",
    "dSetTkSide = {}\n",
    "for n, s in MCsample.iteritems():\n",
    "    dSet[n] = rtnp.root2array(s.skimmed_dir + '/{}_corr.root'.format(cat.name))\n",
    "    dSetTkSide[n] = rtnp.root2array(s.skimmed_dir + '/{}_skip17_corr.root'.format(cat.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run2018D-05May2019promptD-v1_RDntuplizer_B2DstMu_200416\n",
      "HLT_Mu7_IP4_part2_v2 1.28\n",
      "Run2018D-05May2019promptD-v1_RDntuplizer_B2DstMu_200416\n",
      "HLT_Mu7_IP4_part1_v2 1.28\n",
      "Run2018D-05May2019promptD-v1_RDntuplizer_B2DstMu_200416\n",
      "HLT_Mu7_IP4_part4_v2 1.28\n",
      "Run2018D-05May2019promptD-v1_RDntuplizer_B2DstMu_200416\n",
      "HLT_Mu7_IP4_part3_v2 1.28\n",
      "Run2018D-05May2019promptD-v1_RDntuplizer_B2DstMu_200416\n",
      "HLT_Mu7_IP4_part0_v2 1.27\n",
      "Total lumi: 6.39 fb^-1\n"
     ]
    }
   ],
   "source": [
    "if fit_real_data:\n",
    "    creation_date = '200416'\n",
    "    locRD = '../data/cmsRD/skimmed/B2DstMu_{}_{}'.format(creation_date, cat.name)\n",
    "    dSet['data'] = rtnp.root2array(locRD + '_corr.root')\n",
    "    dSetTkSide['data'] = rtnp.root2array(locRD + '_skip17_corr.root')\n",
    "    dataDir = '../data/cmsRD'\n",
    "    datasets_loc = glob(dataDir + '/ParkingBPH*/*RDntuplizer_B2DstMu_{}_CAND.root'.format(creation_date))\n",
    "    lumi_tot = getLumiByTrigger(datasets_loc, cat.trg, verbose=True)\n",
    "    CMS_lumi.integrated_lumi = lumi_tot\n",
    "else:\n",
    "    expectedLumi = {'low':6., 'mid':20., 'high':26., 'single':6.} #fb^-1\n",
    "    lumi_tot = expectedLumi[category]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all the calibrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pileup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = '../data/cmsRD/ParkingBPH{}/Run2018D-05May2019promptD-v1_RDntuplizer_PrescaleVertices_200421_CAND.root'\n",
    "fAuxPileupRD = []\n",
    "\n",
    "hPileupTarget = None\n",
    "\n",
    "for i in range(2, 6):\n",
    "    fAuxPileupRD.append(rt.TFile.Open(loc.format(i), 'READ'))\n",
    "    if hPileupTarget is None:\n",
    "        hPileupTarget = fAuxPileupRD[-1].Get('nVtx/hNvtxPassed'+cat.trg).Clone()\n",
    "    else:\n",
    "        hPileupTarget.Add(fAuxPileupRD[-1].Get('nVtx/hNvtxPassed'+cat.trg))\n",
    "\n",
    "hPileupTarget.Scale(1./hPileupTarget.Integral())\n",
    "\n",
    "fAuxPileupMC = rt.TFile.Open(MCsample['mu'].skimmed_dir + '/{}_corr.root'.format(cat.name), 'READ')\n",
    "hPileupGen = fAuxPileupMC.Get('hAllNvtx')\n",
    "hPileupGen.Scale(1./hPileupGen.Integral())\n",
    "\n",
    "weights = np.ones(hPileupGen.GetNbinsX())\n",
    "s = 0\n",
    "for i in range(weights.shape[0]):\n",
    "    if hPileupGen.GetBinContent(i+1) == 0:\n",
    "        continue\n",
    "    weights[i] = hPileupTarget.GetBinContent(i+1)/hPileupGen.GetBinContent(i+1)\n",
    "    s += hPileupGen.GetBinContent(i+1) * weights[i]\n",
    "\n",
    "weightsPileupMC = weights/s\n",
    "\n",
    "for f in fAuxPileupRD + [fAuxPileupMC]:\n",
    "    f.Close()\n",
    "\n",
    "def getPileupWeights(ds, selection=None):\n",
    "    x = ds['N_vtx']\n",
    "    if not selection is None:\n",
    "        x = x[selection]\n",
    "    return weightsPileupMC[x.astype(np.int)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total norm pre-scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected ratio between RD and MC norm: 2.632 +/- 0.063\n"
     ]
    }
   ],
   "source": [
    "r = np.zeros((3,2))\n",
    "for i, fn in enumerate(glob('../data/calibration/totalRate/ratioB02JPsiKst_*.txt')):\n",
    "    with open(fn, 'r') as faux:\n",
    "        aux = faux.readlines()[0][:-1].split(' ')\n",
    "        r[i] = [float(aux[0]), float(aux[1])]\n",
    "s2 = np.square(r[:,1])\n",
    "num = np.sum(r[:,0]/s2)\n",
    "den = np.sum(1./s2)\n",
    "RDoMC_normRatio = [num/den, np.sqrt(1/den)]\n",
    "print 'Expected ratio between RD and MC norm: {:.3f} +/- {:.3f}'.format(RDoMC_normRatio[0], RDoMC_normRatio[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Branching fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decayBR = pickle.load(open('../data/forcedDecayChannelsFactors.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigger scale factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = '../data/calibration/triggerScaleFactors/'\n",
    "fTriggerSF = rt.TFile.Open(loc + 'HLT_' + cat.trg + '_SF.root', 'READ')\n",
    "hTriggerSF = fTriggerSF.Get('hSF_HLT_' + cat.trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTrgSF(ds, selection=None):\n",
    "    trgSF = np.ones_like(ds['q2'])\n",
    "    trgSFUnc = np.zeros_like(ds['q2'])\n",
    "    if not fastRun:\n",
    "        ptmax = hTriggerSF.GetXaxis().GetXmax() - 0.01\n",
    "        ipmax = hTriggerSF.GetYaxis().GetXmax() - 0.01\n",
    "        etamax = hTriggerSF.GetZaxis().GetXmax() - 0.01\n",
    "        x = np.column_stack((ds['mu_pt'], ds['mu_eta'], ds['mu_sigdxy']))\n",
    "        if not selection is None:\n",
    "            x = x[selection]\n",
    "        for i, (pt, eta, ip) in enumerate(x):\n",
    "            ix = hTriggerSF.GetXaxis().FindBin(min(ptmax, pt))\n",
    "            iy = hTriggerSF.GetYaxis().FindBin(min(ipmax, ip))\n",
    "            iz = hTriggerSF.GetZaxis().FindBin(min(etamax, np.abs(eta)))\n",
    "            trgSF[i] = hTriggerSF.GetBinContent(ix, iy, iz)\n",
    "            ib = hTriggerSF.GetBin(ix, iy, iz)\n",
    "            trgSFUnc[i] = hTriggerSF.GetBinError(ib)\n",
    "            if trgSF[i] == 0:\n",
    "                print pt, ip, np.abs(eta)\n",
    "                raise\n",
    "    # Divide them for the weight so later you can simply multiply back to get the value\n",
    "    up = 1 + trgSFUnc/trgSF\n",
    "    down = 1 - trgSFUnc/trgSF\n",
    "    return trgSF, up, down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muon ID scale factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fMuonIDSF = rt.TFile.Open('../data/calibration/muonIDscaleFactors/Run2018ABCD_SF_MuonID_Jpsi.root', 'READ')\n",
    "hMuonIDSF = fMuonIDSF.Get('NUM_SoftID_DEN_genTracks_pt_abseta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMuonIDSF(ds, selection=None):\n",
    "    muonSF = np.ones_like(ds['q2'])\n",
    "    muonSFUnc = np.zeros_like(ds['q2'])\n",
    "    if not fastRun:\n",
    "        ptmax = hMuonIDSF.GetXaxis().GetXmax() - 0.01\n",
    "        etamax = hMuonIDSF.GetYaxis().GetXmax() - 0.01\n",
    "        x = np.column_stack((ds['MC_mu_pt'], ds['MC_mu_eta']))\n",
    "        if not selection is None:\n",
    "            x = x[selection]\n",
    "        for i, (pt, eta) in enumerate(x):\n",
    "            ix = hMuonIDSF.GetXaxis().FindBin(min(pt, ptmax))\n",
    "            if ix == 0: ix = 1 #Remove underflows (Meaning that the MC matching failed)\n",
    "            iy = hMuonIDSF.GetYaxis().FindBin(min(np.abs(eta), etamax))\n",
    "            muonSF[i] = hMuonIDSF.GetBinContent(ix, iy)\n",
    "            muonSFUnc[i] = hMuonIDSF.GetBinError(hMuonIDSF.GetBin(ix, iy))\n",
    "            if muonSF[i] == 0:\n",
    "                print pt, eta\n",
    "                print ix, iy\n",
    "                raise\n",
    "    up = 1 + muonSFUnc/muonSF\n",
    "    down = 1 - muonSFUnc/muonSF\n",
    "    return muonSF, up, down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B transverse momentum calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = 'Low' if category == 'single' else cat.name\n",
    "cal_pT = pTCalReader(calibration_file='../data/calibration/B0pTspectrum/pwWeights_{}.txt'.format(aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeB0pTweights(ds):\n",
    "    # The denominator (sum of weights) for this weights is not known but it cancel out in the ratio\n",
    "    w = cal_pT.f['C'](ds['MC_B_pt'])\n",
    "    if np.sum(w==0):\n",
    "        print np.sum(w==0)\n",
    "        raise\n",
    "    up = cal_pT.f['Up'](ds['MC_B_pt'])/w\n",
    "    down = cal_pT.f['Down'](ds['MC_B_pt'])/w\n",
    "    return w, up, down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create MC histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------> tau <-------------\n",
      "N tot selected: 28.3k\n",
      "N tot expected (before weights): 6.0k\n",
      "Including pileup reweighting\n",
      "Including trigger corrections\n",
      "Including muon ID corrections\n",
      "Including B0 pT corrections\n",
      "Average weight: 1.058\n",
      "Including FF corrections (Hammer)\n",
      "Computing total weights\n",
      "N tot expected (after weights): 6.5k\n",
      "\n",
      "-----------> DstPipPi0 <-------------\n",
      "N tot selected: 14.5k\n",
      "N tot expected (before weights): 0.2k\n",
      "Including pileup reweighting\n",
      "Including trigger corrections\n",
      "Including muon ID corrections\n",
      "Computing total weights\n",
      "N tot expected (after weights): 0.2k\n",
      "\n",
      "-----------> DstPi0 <-------------\n",
      "N tot selected: 36.1k\n",
      "N tot expected (before weights): 3.8k\n",
      "Including pileup reweighting\n",
      "Including trigger corrections\n",
      "Including muon ID corrections\n",
      "Including B0 pT corrections\n",
      "Average weight: 1.039\n",
      "Computing total weights\n",
      "N tot expected (after weights): 4.1k\n",
      "\n",
      "-----------> mu <-------------\n",
      "N tot selected: 230.3k\n",
      "N tot expected (before weights): 118.5k\n",
      "Including pileup reweighting\n",
      "Including trigger corrections\n",
      "Including muon ID corrections\n",
      "Including B0 pT corrections\n",
      "Average weight: 1.000\n",
      "Including FF corrections (Hammer)\n",
      "Computing total weights\n",
      "N tot expected (after weights): 123.3k\n",
      "\n",
      "-----------> DstPip <-------------\n",
      "N tot selected: 21.7k\n",
      "N tot expected (before weights): 4.4k\n",
      "Including pileup reweighting\n",
      "Including trigger corrections\n",
      "Including muon ID corrections\n",
      "Computing total weights\n",
      "N tot expected (after weights): 4.7k\n",
      "\n",
      "-----------> DstPipPim <-------------\n",
      "N tot selected: 5.4k\n",
      "N tot expected (before weights): 0.3k\n",
      "Including pileup reweighting\n",
      "Including trigger corrections\n",
      "Including muon ID corrections\n",
      "Including B0 pT corrections\n",
      "Average weight: 0.974\n",
      "Computing total weights\n",
      "N tot expected (after weights): 0.3k\n",
      "\n",
      "-----------> Hc <-------------\n",
      "N tot selected: 9.7k\n",
      "N tot expected (before weights): 1.1k\n",
      "Including pileup reweighting\n",
      "Including trigger corrections\n",
      "Including muon ID corrections\n",
      "Including B0 pT corrections\n",
      "Average weight: 1.063\n",
      "Computing total weights\n",
      "N tot expected (after weights): 1.2k\n",
      "\n",
      "-----------> DstPi0Pi0 <-------------\n",
      "N tot selected: 29.8k\n",
      "N tot expected (before weights): 0.6k\n",
      "Including pileup reweighting\n",
      "Including trigger corrections\n",
      "Including muon ID corrections\n",
      "Including B0 pT corrections\n",
      "Average weight: 1.030\n",
      "Computing total weights\n",
      "N tot expected (after weights): 0.6k\n"
     ]
    }
   ],
   "source": [
    "histo = {}\n",
    "for n, ds in dSet.iteritems():\n",
    "    if n == 'data': continue\n",
    "    print '\\n----------->', n, '<-------------'\n",
    "    sMC = MCsample[n]\n",
    "    \n",
    "    nTotSelected = ds['q2'].shape[0]\n",
    "    print 'N tot selected: {:.1f}k'.format(1e-3*nTotSelected)\n",
    "    nGenExp = sMC.effMCgen['xsec'][0] * lumi_tot * RDoMC_normRatio[0]\n",
    "    eff = [1, 0]\n",
    "    for f, df in [sMC.effMCgen['effGEN'], decayBR[n], sMC.effCand['effCAND'], sMC.getSkimEff(cat.name+'_corr')]:\n",
    "        eff[0] *= f\n",
    "        eff[1] += np.square(df/f)\n",
    "    eff[1] = eff[0] * np.sqrt(eff[1])\n",
    "    nTotExp = nGenExp*eff[0]\n",
    "    print 'N tot expected (before weights): {:.1f}k'.format(1e-3*nTotExp)\n",
    "    \n",
    "    wVar = {}\n",
    "    weights = {}\n",
    "    \n",
    "    print 'Including pileup reweighting'\n",
    "    weights['pileup'] = getPileupWeights(ds)\n",
    "    print 'Including trigger corrections'\n",
    "    weights['trgSF'], wVar['trgSFUp'], wVar['trgSFDown'] = computeTrgSF(ds)\n",
    "    print 'Including muon ID corrections'\n",
    "    weights['muonIdSF'], wVar['muonIdSFUp'], wVar['muonIdSFDown'] = computeMuonIDSF(ds)\n",
    "    if n in ['mu', 'tau', 'Hc','DstPi0', 'DstPipPim', 'DstPi0Pi0']: #B0 dominated final states (probably we should do something about B+ too)\n",
    "        print 'Including B0 pT corrections'\n",
    "        weights['B0pT'], wVar['B0pTUp'], wVar['B0pTDown'] = computeB0pTweights(ds)\n",
    "        print 'Average weight: {:.3f}'.format(np.average(weights['B0pT']))\n",
    "    \n",
    "    # Hammer corrections to the FF\n",
    "    if n in ['mu', 'tau']:\n",
    "        print 'Including FF corrections (Hammer)'\n",
    "        weights['B2DstCLN'] = ds['wh_CLNCentral']*sMC.effCand['rate_den']/sMC.effCand['rate_Central']\n",
    "        for nPar in ['R0', 'R1', 'R2', 'RhoSq']:\n",
    "            for var in ['Up', 'Down']:\n",
    "                tag = 'CLN' + nPar + var\n",
    "                wVar['B2Dst'+tag] = ds['wh_'+tag]/sMC.effCand['rate_' + nPar + var]\n",
    "                wVar['B2Dst'+tag] *= sMC.effCand['rate_Central']/ds['wh_CLNCentral']\n",
    "    \n",
    "    print 'Computing total weights'\n",
    "    weightsCentral = np.ones_like(ds['q2'])\n",
    "    for w in weights.values(): weightsCentral *= w\n",
    "    print 'N tot expected (after weights): {:.1f}k'.format(1e-3*nTotExp*np.sum(weightsCentral)/nTotSelected)\n",
    "    wVar[''] = np.ones_like(weightsCentral)\n",
    "    \n",
    "    for i_q2 in range(len(binning['q2'])-1):\n",
    "        q2_l = binning['q2'][i_q2]\n",
    "        q2_h = binning['q2'][i_q2 + 1]\n",
    "        sel_q2 = np.logical_and(ds['q2'] > q2_l, ds['q2'] < q2_h)\n",
    "        for var in ['M2_miss', 'Est_mu']:\n",
    "            cat_name = var+'_q2bin'+str(i_q2)\n",
    "            \n",
    "            if not cat_name in histo.keys():\n",
    "                histo[cat_name] = {}\n",
    "            \n",
    "            for name_wVar, v_wVar in wVar.iteritems():\n",
    "                h_name = n\n",
    "                if not name_wVar == '':\n",
    "                    h_name += '__' + name_wVar\n",
    "                w = weightsCentral*v_wVar\n",
    "                scale = nTotExp/nTotSelected\n",
    "                histo[cat_name][h_name] = create_TH1D(\n",
    "                                                      ds[var][sel_q2], \n",
    "                                                      name=h_name, title=h_name, \n",
    "                                                      binning=binning[var][i_q2], \n",
    "                                                      opt='underflow,overflow',\n",
    "                                                      weights=w[sel_q2], scale_histo=scale,\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single track side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectionSingleAddTkMassHad(ds):\n",
    "    sel = np.logical_and(ds['N_goodAddTks'] == 1, ds['tkCharge_0'] > 0)\n",
    "    return np.logical_and(np.abs(ds['tkMassHad_0'] - 2.43) < 0.3, sel)\n",
    "\n",
    "def selectionSingleAddTkMassVis(ds):\n",
    "    sel = np.logical_and(ds['tkCharge_0']*ds['tkCharge_1'] < 0, ds['N_goodAddTks'] == 2)\n",
    "    sel = np.logical_and(ds['tkMassVis12'] < 5.3, sel)\n",
    "#     sel = np.logical_and(ds['N_goodAddTks'] == 1, ds['tkCharge_0'] < 0)\n",
    "    return sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------> tau <-------------\n",
      "Including pileup reweighting\n",
      "Including trigger corrections\n",
      "Including muon ID corrections\n",
      "Including B0 pT corrections\n",
      "Average weight: 1.058\n",
      "Including FF corrections (Hammer)\n",
      "Computing total weights\n",
      "N tot selected Mass Had: 1040\n",
      "N tot expected Mass Had (before weights): 220\n",
      "N tot expected Mass Had (after weights): 234\n",
      "N tot selected Mass Vis: 79\n",
      "N tot expected Mass Vis (before weights): 17\n",
      "N tot expected Mass Vis (after weights): 18\n",
      "\n",
      "-----------> DstPipPi0 <-------------\n",
      "Including pileup reweighting\n",
      "Including trigger corrections\n",
      "Including muon ID corrections\n",
      "Computing total weights\n",
      "N tot selected Mass Had: 31099\n",
      "N tot expected Mass Had (before weights): 461\n",
      "N tot expected Mass Had (after weights): 488\n",
      "N tot selected Mass Vis: 1408\n",
      "N tot expected Mass Vis (before weights): 21\n",
      "N tot expected Mass Vis (after weights): 22\n",
      "\n",
      "-----------> DstPi0 <-------------\n",
      "Including pileup reweighting\n",
      "Including trigger corrections\n",
      "Including muon ID corrections\n",
      "Including B0 pT corrections\n",
      "Average weight: 1.039\n",
      "Computing total weights\n",
      "N tot selected Mass Had: 1530\n",
      "N tot expected Mass Had (before weights): 161\n",
      "N tot expected Mass Had (after weights): 171\n",
      "N tot selected Mass Vis: 195\n",
      "N tot expected Mass Vis (before weights): 21\n",
      "N tot expected Mass Vis (after weights): 22\n",
      "\n",
      "-----------> mu <-------------\n",
      "Including pileup reweighting\n",
      "Including trigger corrections\n",
      "Including muon ID corrections\n",
      "Including B0 pT corrections\n",
      "Average weight: 0.999\n",
      "Including FF corrections (Hammer)\n",
      "Computing total weights\n",
      "N tot selected Mass Had: 10569\n",
      "N tot expected Mass Had (before weights): 5438\n",
      "N tot expected Mass Had (after weights): 5389\n",
      "N tot selected Mass Vis: 927\n",
      "N tot expected Mass Vis (before weights): 477\n",
      "N tot expected Mass Vis (after weights): 470\n",
      "\n",
      "-----------> DstPip <-------------\n",
      "Including pileup reweighting\n",
      "Including trigger corrections\n",
      "Including muon ID corrections\n",
      "Computing total weights\n",
      "N tot selected Mass Had: 34858\n",
      "N tot expected Mass Had (before weights): 7102\n",
      "N tot expected Mass Had (after weights): 7462\n",
      "N tot selected Mass Vis: 1589\n",
      "N tot expected Mass Vis (before weights): 324\n",
      "N tot expected Mass Vis (after weights): 332\n",
      "\n",
      "-----------> DstPipPim <-------------\n",
      "Including pileup reweighting\n",
      "Including trigger corrections\n",
      "Including muon ID corrections\n",
      "Including B0 pT corrections\n",
      "Average weight: 1.025\n",
      "Computing total weights\n",
      "N tot selected Mass Had: 8421\n",
      "N tot expected Mass Had (before weights): 431\n",
      "N tot expected Mass Had (after weights): 447\n",
      "N tot selected Mass Vis: 16540\n",
      "N tot expected Mass Vis (before weights): 846\n",
      "N tot expected Mass Vis (after weights): 932\n",
      "\n",
      "-----------> Hc <-------------\n",
      "Including pileup reweighting\n",
      "Including trigger corrections\n",
      "Including muon ID corrections\n",
      "Including B0 pT corrections\n",
      "Average weight: 1.076\n",
      "Computing total weights\n",
      "N tot selected Mass Had: 1601\n",
      "N tot expected Mass Had (before weights): 183\n",
      "N tot expected Mass Had (after weights): 202\n",
      "N tot selected Mass Vis: 3001\n",
      "N tot expected Mass Vis (before weights): 343\n",
      "N tot expected Mass Vis (after weights): 404\n",
      "\n",
      "-----------> DstPi0Pi0 <-------------\n",
      "Including pileup reweighting\n",
      "Including trigger corrections\n",
      "Including muon ID corrections\n",
      "Including B0 pT corrections\n",
      "Average weight: 1.029\n",
      "Computing total weights\n",
      "N tot selected Mass Had: 1384\n",
      "N tot expected Mass Had (before weights): 26\n",
      "N tot expected Mass Had (after weights): 28\n",
      "N tot selected Mass Vis: 135\n",
      "N tot expected Mass Vis (before weights): 3\n",
      "N tot expected Mass Vis (after weights): 3\n"
     ]
    }
   ],
   "source": [
    "histo['SingleAddTkMassHad'] = {}\n",
    "histo['SingleAddTkMassVis'] = {}\n",
    "\n",
    "for n, ds in dSetTkSide.iteritems():\n",
    "    if n == 'data': continue\n",
    "    print '\\n----------->', n, '<-------------'\n",
    "    sMC = MCsample[n]    \n",
    "    wVar = {}\n",
    "    weights = {}\n",
    "    \n",
    "    print 'Including pileup reweighting'\n",
    "    weights['pileup'] = getPileupWeights(ds)\n",
    "    print 'Including trigger corrections'\n",
    "    weights['trgSF'], wVar['trgSFUp'], wVar['trgSFDown'] = computeTrgSF(ds)\n",
    "    print 'Including muon ID corrections'\n",
    "    weights['muonIdSF'], wVar['muonIdSFUp'], wVar['muonIdSFDown'] = computeMuonIDSF(ds)\n",
    "    if n in ['mu', 'tau', 'Hc','DstPi0', 'DstPipPim', 'DstPi0Pi0']: #B0 dominated final states (probably we should do something about B+ too)\n",
    "        print 'Including B0 pT corrections'\n",
    "        weights['B0pT'], wVar['B0pTUp'], wVar['B0pTDown'] = computeB0pTweights(ds)\n",
    "        print 'Average weight: {:.3f}'.format(np.average(weights['B0pT']))\n",
    "    \n",
    "    # Hammer corrections to the FF\n",
    "    if n in ['mu', 'tau']:\n",
    "        print 'Including FF corrections (Hammer)'\n",
    "        weights['B2DstCLN'] = ds['wh_CLNCentral']*sMC.effCand['rate_den']/sMC.effCand['rate_Central']\n",
    "        for nPar in ['R0', 'R1', 'R2', 'RhoSq']:\n",
    "            for var in ['Up', 'Down']:\n",
    "                tag = 'CLN' + nPar + var\n",
    "                wVar['B2Dst'+tag] = ds['wh_'+tag]/sMC.effCand['rate_' + nPar + var]\n",
    "                wVar['B2Dst'+tag] *= sMC.effCand['rate_Central']/ds['wh_CLNCentral']\n",
    "    \n",
    "    print 'Computing total weights'\n",
    "    weightsCentral = np.ones_like(ds['q2'])\n",
    "    for w in weights.values(): weightsCentral *= w\n",
    "    wVar[''] = np.ones_like(weightsCentral)\n",
    "    \n",
    "    nGenExp = sMC.effMCgen['xsec'][0] * lumi_tot * RDoMC_normRatio[0]\n",
    "    eff = [1, 0]\n",
    "    for f, df in [sMC.effMCgen['effGEN'], \n",
    "                  decayBR[n], \n",
    "                  sMC.effCand['effCAND'], \n",
    "                  sMC.getSkimEff(cat.name+'_skip17_corr'),\n",
    "                 ]:\n",
    "        eff[0] *= f\n",
    "        eff[1] += np.square(df/f)\n",
    "    eff[1] = eff[0] * np.sqrt(eff[1])\n",
    "    nTotExp = nGenExp*eff[0]\n",
    "    \n",
    "    selMassHad = selectionSingleAddTkMassHad(ds)\n",
    "    nTotSelMassHad = float(np.sum(selMassHad))\n",
    "    print 'N tot selected Mass Had: {:.0f}'.format(nTotSelMassHad)\n",
    "    nTotExpMassHad = nTotExp * nTotSelMassHad / selMassHad.shape[0]\n",
    "    print 'N tot expected Mass Had (before weights): {:.0f}'.format(nTotExpMassHad)\n",
    "    nAux = nTotExpMassHad*np.sum(weightsCentral[selMassHad])/nTotSelMassHad\n",
    "    print 'N tot expected Mass Had (after weights): {:.0f}'.format(nAux)\n",
    "    scaleMassHad = nTotExpMassHad/nTotSelMassHad\n",
    "    \n",
    "    selMassVis = selectionSingleAddTkMassVis(ds)\n",
    "    nTotSelMassVis = float(np.sum(selMassVis))\n",
    "    print 'N tot selected Mass Vis: {:.0f}'.format(nTotSelMassVis)\n",
    "    nTotExpMassVis = nTotExp * nTotSelMassVis / selMassVis.shape[0]\n",
    "    print 'N tot expected Mass Vis (before weights): {:.0f}'.format(nTotExpMassVis)\n",
    "    nAux = nTotExpMassVis*np.sum(weightsCentral[selMassVis])/nTotSelMassVis\n",
    "    print 'N tot expected Mass Vis (after weights): {:.0f}'.format(nAux)\n",
    "    scaleMassVis = nTotExpMassVis/nTotSelMassVis\n",
    "    \n",
    "            \n",
    "    for name_wVar, v_wVar in wVar.iteritems():\n",
    "        h_name = n\n",
    "        if not name_wVar == '':\n",
    "            h_name += '__' + name_wVar\n",
    "        w = weightsCentral*v_wVar\n",
    "        histo['SingleAddTkMassHad'][h_name] = create_TH1D(\n",
    "                                              ds['tkMassHad_0'][selMassHad], \n",
    "                                              name=h_name, title=h_name, \n",
    "                                              binning=binning['tkMassHad_0'],\n",
    "                                              opt='underflow,overflow',\n",
    "                                              weights=w[selMassHad], scale_histo=scale,\n",
    "                                              )\n",
    "        histo['SingleAddTkMassVis'][h_name] = create_TH1D(\n",
    "                                              ds['tkMassVis12'][selMassVis], \n",
    "                                              name=h_name, title=h_name, \n",
    "                                              binning=binning['tkMassVis_0'],\n",
    "                                              opt='underflow,overflow',\n",
    "                                              weights=w[selMassVis], scale_histo=scale,\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create (pseudo-)data histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N observed data: 162.7k\n"
     ]
    }
   ],
   "source": [
    "if fit_real_data:\n",
    "    ds = dSet['data']\n",
    "    print 'N observed data: {:.1f}k'.format(1e-3*ds['q2'].shape[0])\n",
    "    for i_q2 in range(len(binning['q2'])-1):\n",
    "        q2_l = binning['q2'][i_q2]\n",
    "        q2_h = binning['q2'][i_q2 + 1]\n",
    "        sel_q2 = np.logical_and(ds['q2'] > q2_l, ds['q2'] < q2_h)\n",
    "        for var in ['M2_miss', 'Est_mu']:\n",
    "            cat_name = var+'_q2bin'+str(i_q2)     \n",
    "            histo[cat_name]['data'] = create_TH1D(\n",
    "                                                  ds[var][sel_q2], \n",
    "                                                  name='data_obs', title='Data Obs',\n",
    "                                                  binning=binning[var][i_q2],\n",
    "                                                  opt='underflow,overflow'\n",
    "                                                 )\n",
    "    ds = dSetTkSide['data']\n",
    "    histo['SingleAddTkMassHad']['data'] = create_TH1D(\n",
    "                                          ds['tkMassHad_0'][selectionSingleAddTkMassHad(ds)], \n",
    "                                          name='data_obs', title='Data Obs',\n",
    "                                          binning=binning['tkMassHad_0'], \n",
    "                                          opt='underflow,overflow'\n",
    "                                          )\n",
    "    \n",
    "    histo['SingleAddTkMassVis']['data'] = create_TH1D(\n",
    "                                              ds['tkMassVis12'][selectionSingleAddTkMassVis(ds)], \n",
    "                                              name='data_obs', title='Data Obs',\n",
    "                                              binning=binning['tkMassVis_0'],\n",
    "                                              opt='underflow,overflow',\n",
    "                                              )\n",
    "else:\n",
    "    for i_q2 in range(len(binning['q2'])-1):\n",
    "        for var in ['M2_miss', 'Est_mu']:\n",
    "            cat_name = var+'_q2bin'+str(i_q2)\n",
    "            \n",
    "            h = create_TH1D(np.array([0, 0]), name='data_obs', title='Data Obs', binning=binning[var][i_q2])\n",
    "            h.Reset()\n",
    "            for n, hMC in histo[cat_name].iteritems():\n",
    "                if not '__' in n and not n == 'data':\n",
    "                    scale = SM_RDst if 'tau' in n else 1.\n",
    "                    h.Add(hMC, scale)\n",
    "            h.Sumw2(0)\n",
    "            for i in range(1, h.GetNbinsX()+1):\n",
    "                h.SetBinContent(i, np.around(h.GetBinContent(i)))\n",
    "            h.Sumw2()\n",
    "            histo[cat_name]['data'] = h\n",
    "            \n",
    "    for var in ['Had', 'Vis']:\n",
    "        h = create_TH1D(np.array([0, 0]), name='data_obs', title='Data Obs', \n",
    "                        binning=binning['tkMass{}_0'.format(var)])\n",
    "        h.Reset()\n",
    "        for n, hMC in histo['SingleAddTkMass{}'.format(var)].iteritems():\n",
    "            if not '__' in n and not n == 'data':\n",
    "                scale = SM_RDst if 'tau' in n else 1.\n",
    "                h.Add(hMC, scale)\n",
    "        h.Sumw2(0)\n",
    "        for i in range(1, h.GetNbinsX()+1):\n",
    "            h.SetBinContent(i, np.around(h.GetBinContent(i)))\n",
    "        h.Sumw2()\n",
    "        histo['SingleAddTkMass{}'.format(var)]['data'] = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CMS_lumi.integrated_lumi = lumi_tot\n",
    "scale_dic = {'tau': SM_RDst\n",
    "            }\n",
    "\n",
    "cSig = plot_gridVarQ2(CMS_lumi, binning, histo, scale_dic=scale_dic, min_y=1, logy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cHad = plot_SingleAddTkMassHad(CMS_lumi, histo, scale_dic=scale_dic, min_y=1, logy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cVis = plot_SingleAddTkMassVis(CMS_lumi, histo, scale_dic=scale_dic, min_y=1, logy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histo_file_dir = '../data/_root/histos4combine/'\n",
    "if not os.path.isdir(histo_file_dir):\n",
    "    os.makedirs(histo_file_dir)\n",
    "histo_file_loc = {}\n",
    "for cat_name, h_dic in histo.iteritems():\n",
    "    histo_file_loc[cat_name] = histo_file_dir+'{}_{}.root'.format(card_name, cat_name)\n",
    "    tf = rt.TFile(histo_file_loc[cat_name], 'recreate')\n",
    "    for v in h_dic.values():\n",
    "        v.Write()\n",
    "    tf.Close()\n",
    "    \n",
    "del dSet, dSetTkSide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_processes = ['tau', 'mu']\n",
    "bkg_processes = ['Hc', 'Dstst']\n",
    "processes = sig_processes + bkg_processes\n",
    "nProc = len(processes)\n",
    "categories = np.sort([k for k in histo.keys()])\n",
    "nCat = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T01:36:12.367638Z",
     "start_time": "2019-05-14T01:36:12.352769Z"
    }
   },
   "outputs": [],
   "source": [
    "card_location = 'cards/{}.txt'.format(card_name)\n",
    "fc = open(card_location, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T01:36:12.392125Z",
     "start_time": "2019-05-14T01:36:12.371738Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of different categories\n",
    "card = 'imax *\\n'\n",
    "# number of processes minus one\n",
    "card += 'jmax {}\\n'.format(len(processes)-1)\n",
    "# number of nuissance parameters\n",
    "card += 'kmax *\\n'\n",
    "card += '--------------------------------------------------------------\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T01:36:12.392125Z",
     "start_time": "2019-05-14T01:36:12.371738Z"
    }
   },
   "outputs": [],
   "source": [
    "# shape file location\n",
    "for k in categories:\n",
    "    card += 'shapes * {} {} $PROCESS $PROCESS__$SYSTEMATIC\\n'.format(k, histo_file_loc[k])\n",
    "card += '--------------------------------------------------------------\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T01:36:12.392125Z",
     "start_time": "2019-05-14T01:36:12.371738Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of events observed\n",
    "card += 'bin ' + ' '.join(categories) + '\\n'\n",
    "obs = map(lambda k: '{:.0f}'.format(histo[k]['data'].Integral()), categories)\n",
    "obs = ' '.join(obs)\n",
    "card += 'observation ' + obs + '\\n'\n",
    "card += '--------------------------------------------------------------\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T01:36:12.392125Z",
     "start_time": "2019-05-14T01:36:12.371738Z"
    }
   },
   "outputs": [],
   "source": [
    "# MC expected events\n",
    "aux_bin = ''\n",
    "aux_proc_name = ''\n",
    "aux_proc_id = ''\n",
    "aux_proc_rate = ''\n",
    "for c, p in itertools.product(categories, processes):\n",
    "    aux_bin += ' '+c\n",
    "    aux_proc_name += ' '+p\n",
    "    aux_proc_id += ' '+str(np.argmax(np.array(processes) == p))\n",
    "    aux_proc_rate += ' {:.2f}'.format(histo[c][p].Integral())\n",
    "    \n",
    "card += 'bin' + aux_bin + '\\n'\n",
    "card += 'process' + aux_proc_name + '\\n'\n",
    "# Zero or negative for sig and positive for bkg\n",
    "card += 'process' + aux_proc_id + '\\n'\n",
    "# Expected rate\n",
    "card += 'rate' + aux_proc_rate + '\\n'\n",
    "card += '--------------------------------------------------------------\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systematic uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pp -> bb cros-section * luminosity\n",
    "card += 'xsecpp2bbXlumi lnN' + ' 1.9'*nProc*nCat + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hadronization fraction of B0\n",
    "aux = ''\n",
    "for n in processes:\n",
    "    if n in ['tau', 'mu', 'Hc']: aux += ' 2'\n",
    "    else: aux += ' -'\n",
    "card += 'b2B0Had lnN' + aux*nCat + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hadronization fraction of B+\n",
    "# aux = ''\n",
    "# for n in processes:\n",
    "#     if n in ['Dstst']: aux += ' 3'\n",
    "#     else: aux += ' -'\n",
    "# card += 'b2BpHad lnN' + aux*nCat + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection efficiencies\n",
    "for n in processes:\n",
    "    if n == 'tau': continue\n",
    "    val = ' {:.3f}'.format(1+decayBR[n][1])\n",
    "    if n == 'Dstst':\n",
    "        val = ' 3.'\n",
    "    aux = ''\n",
    "    for nn in processes:\n",
    "        if nn == n: aux += val\n",
    "        else: aux += ' -'\n",
    "    card += n + 'Eff lnN' + aux*nCat + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux = ''\n",
    "# for c in categories:\n",
    "#     if not 'AddTk' in c:\n",
    "#         aux += ' -'*nProc\n",
    "#     else:\n",
    "#         for p in processes:\n",
    "#             if p == 'mu':\n",
    "#                 aux += ' 3.'\n",
    "#             else:\n",
    "#                 aux += ' -'\n",
    "# card += 'muAddTrak lnN' + aux + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card += '--------------------------------------------------------------\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape Systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card += 'trgSF shape' + ' 1.'*nProc*nCat + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card += 'muonIdSF shape' + ' 1.'*nProc*nCat + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B0 pT spectrum\n",
    "aux = ''\n",
    "for p in processes:\n",
    "    if p in ['tau', 'mu', 'Hc']:\n",
    "        aux += ' 1.'\n",
    "    else:\n",
    "        aux += ' -'\n",
    "card += 'B0pT shape' + aux*nCat + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Factors from Hammer\n",
    "for n_pFF in ['R0', 'R1', 'R2', 'RhoSq']:\n",
    "    aux = ''\n",
    "    for p in processes:\n",
    "        if p in ['tau', 'mu']:\n",
    "            aux += ' 1.'\n",
    "        else:\n",
    "            aux += ' -'\n",
    "    card += 'B2DstCLN{} shape'.format(n_pFF) + aux*nCat + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC statistic systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card += '* autoMCStats 0 1 1\\n'\n",
    "card += '--------------------------------------------------------------\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining groups of systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoMCStats group = defined by default when using autoMCStats\n",
    "aux_FF = ' '.join(['B2DstCLN'+n for n in ['R0', 'R1', 'R2', 'RhoSq']])\n",
    "card += 'B2DstFF group = ' + aux_FF + '\\n'\n",
    "\n",
    "# card += 'normMC group = xsecpp2bbXlumi b2B0Had b2BpHad\\n'\n",
    "card += 'normMC group = xsecpp2bbXlumi b2B0Had\\n'\n",
    "\n",
    "card += 'effMC group = ' + ' '.join([n+'Eff' for n in processes if not n == 'tau']) + '\\n'\n",
    "\n",
    "card += 'allShape group = trgSF muonIdSF B0pT ' + aux_FF + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T01:36:12.412621Z",
     "start_time": "2019-05-14T01:36:12.396482Z"
    }
   },
   "outputs": [],
   "source": [
    "print card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T01:36:12.432835Z",
     "start_time": "2019-05-14T01:36:12.416805Z"
    }
   },
   "outputs": [],
   "source": [
    "fc.write(card)\n",
    "fc.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = 'results/' + card_name\n",
    "\n",
    "if os.path.isdir(outdir):\n",
    "    os.system('rm -rf ' + outdir)\n",
    "os.system('mkdir ' + outdir);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmd = 'text2workspace.py ' + card_location \n",
    "cmd += ' -o ' + card_location.replace('.txt', '.root')\n",
    "cmd += ' --no-b-only'\n",
    "cmd += ' --verbose 1'\n",
    "# cmd += ' --no-wrappers'\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Maximum Likelyhood fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seedMLf = '6741'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmd = 'combine -M FitDiagnostics'\n",
    "cmd += ' --robustFit 1 --cminDefaultMinimizerStrategy 0 --X-rtd MINIMIZER_analytic'\n",
    "cmd += ' --skipBOnlyFit'\n",
    "cmd += ' --seed ' + seedMLf\n",
    "cmd += ' -d ' + card_location.replace('.txt', '.root')\n",
    "cmd += ' -D ' + histo[histo.keys()[0]]['data'].GetName()\n",
    "cmd += ' --setParameters r={:.2f}'.format(SM_RDst)\n",
    "cmd += ' --setParameterRanges r=0.001,5'\n",
    "cmd += ' -n {}'.format(card_name)\n",
    "cmd += ' --out ' + outdir\n",
    "cmd += ' --saveShapes --saveWithUncertainties --saveNormalizations'\n",
    "cmd += ' --plots'\n",
    "cmd += ' --verbose 1'\n",
    "\n",
    "print cmd\n",
    "os.system(cmd)\n",
    "os.system('mv combine_logger.out ' + outdir + '/')\n",
    "os.system('mv ./*.root ' + outdir + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ur.open(glob(outdir + '/higgsCombine{}.FitDiagnostics.mH120.{}.root'.format(card_name, seedMLf))[0])\n",
    "c, d, u, _ = f['limit']['limit'].array()\n",
    "print 'R(D*) = {:.3f} +{:.3f}/-{:.3f} [{:.1f} %]'.format(c, u-c, c-d, 100*(u-d)*0.5/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get post-fit shapes\n",
    "f = rt.TFile.Open(glob(outdir + '/fitDiagnostics{}.root'.format(card_name))[0], 'READ')\n",
    "fd = f.shapes_fit_s\n",
    "\n",
    "histo_postfit = {}\n",
    "for cat, h_dic in histo.iteritems():\n",
    "    histo_postfit[cat] = {}\n",
    "    for n, h in h_dic.iteritems():\n",
    "        if '__' in n:\n",
    "            continue\n",
    "        h_post = h.Clone(h.GetName() + '_postfit')\n",
    "        if 'data' in n:\n",
    "            h_fit = fd.Get(cat+'/total')\n",
    "            h_data = h.Clone(h.GetName() + '_data')\n",
    "            for i in range(1, h_post.GetNbinsX()+1):\n",
    "                h_post.SetBinContent(i, h_fit.GetBinContent(i))\n",
    "                h_post.SetBinError(i, h_fit.GetBinError(i))     \n",
    "            \n",
    "            histo_postfit[cat]['total'] = h_post\n",
    "            histo_postfit[cat][n] = h_data\n",
    "        else:\n",
    "            h_fit = fd.Get(cat+'/'+n)\n",
    "            for i in range(1, h_post.GetNbinsX()+1):\n",
    "                h_post.SetBinContent(i, h_fit.GetBinContent(i))\n",
    "                h_post.SetBinError(i, h_fit.GetBinError(i)) \n",
    "\n",
    "            histo_postfit[cat][n] = h_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ccPostSig = plot_gridVarQ2(CMS_lumi, binning, histo_postfit, scale_dic={}, draw_pulls=True, pulls_ylim=[0.9, 1.1])\n",
    "ccPostSig.SaveAs('gridPlot_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccPostHad = plot_SingleAddTkMassHad(CMS_lumi, histo_postfit, scale_dic={}, draw_pulls=True, pulls_ylim=[0.8, 1.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccPostVis = plot_SingleAddTkMassVis(CMS_lumi, histo_postfit, scale_dic={}, draw_pulls=True, pulls_ylim=[0.8, 1.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h2 = f.Get('covariance_fit_s')\n",
    "rt.gStyle.SetPaintTextFormat('.1f')\n",
    "\n",
    "N = h2.GetNbinsX()\n",
    "n=15\n",
    "\n",
    "h2.GetXaxis().SetRange(1, n)\n",
    "h2.GetYaxis().SetRangeUser(N-n, N)\n",
    "h2.SetMarkerSize(1.3)\n",
    "CC = drawOnCMSCanvas(CMS_lumi, [h2, h2], ['colz', 'text same'], tag='tl', mL=0.22, mR=0.15, mB=0.18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmd = 'python diffNuisances.py '.format(os.environ['CMSSW_BASE'])\n",
    "cmd += glob(outdir + '/fitDiagnostics{}.root'.format(card_name))[0]\n",
    "cmd += ' --skipFitB'\n",
    "cmd += ' -g {}/nuisance_pull.root'.format(outdir)\n",
    "print cmd\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fastRun:\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run likelyhood scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUncertainty(name, verbose=True):\n",
    "    f = ur.open(name)\n",
    "    r_arr = f['limit']['r'].array()\n",
    "    nll_arr = f['limit']['deltaNLL'].array()\n",
    "    c = r_arr[0]\n",
    "    r_u = r_arr[r_arr>r_arr[0]]\n",
    "    nll_u = nll_arr[r_arr>r_arr[0]]\n",
    "    f_u = interp1d(nll_u, r_u, 'quadratic')\n",
    "    u = f_u(0.5)\n",
    "    r_l = r_arr[r_arr<r_arr[0]]\n",
    "    nll_l = nll_arr[r_arr<r_arr[0]]\n",
    "    f_l = interp1d(nll_l, r_l, 'quadratic')\n",
    "    l = f_l(0.5)\n",
    "    if verbose:\n",
    "        print '----------------------------------'\n",
    "        print 'R(D*) = {:.3f} +{:.3f}/-{:.3f} [{:.1f} %]'.format(c, u-c, c-d, 100*(u-d)*0.5/c)\n",
    "        print 'Sigma = {:.3f}'.format((u-l)*0.5)\n",
    "        print '----------------------------------\\n'\n",
    "    return c, c-l, u-c, (u-l)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'combine'\n",
    "cmd += ' -M MultiDimFit'\n",
    "cmd += ' --algo grid --points=100'\n",
    "cmd += ' --robustFit 1'\n",
    "cmd += ' -d ' + card_location.replace('.txt', '.root')\n",
    "cmd += ' -D ' + histo[histo.keys()[0]]['data'].GetName()\n",
    "cmd += ' --X-rtd MINIMIZER_analytic'\n",
    "cmd += ' --rMin={:.4f} --rMax={:.4f}'.format(c - 5*(c-d), c + 5*(u-c))\n",
    "cmd += ' -n {}_nominal'.format(card_name)\n",
    "cmd += ' --verbose -1'\n",
    "print cmd\n",
    "os.system(cmd)\n",
    "cmd = 'plot1DScan.py higgsCombine{}_nominal.MultiDimFit.mH120.root'.format(card_name)\n",
    "cmd += '; mv scan.png scan_nominal.png'\n",
    "os.system(cmd)\n",
    "res_nominal = getUncertainty('higgsCombine{}_nominal.MultiDimFit.mH120.root'.format(card_name))\n",
    "display(Image(filename='scan_nominal.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainy breakdown by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'combine -M MultiDimFit --algo none'\n",
    "cmd += ' --cminDefaultMinimizerStrategy=2 --robustFit 1 --X-rtd MINIMIZER_analytic'\n",
    "cmd += ' -d ' + card_location.replace('.txt', '.root')\n",
    "cmd += ' -D ' + histo[histo.keys()[0]]['data'].GetName()\n",
    "cmd += ' --setParameters r={:.2f}'.format(SM_RDst)\n",
    "cmd += ' --setParameterRanges r=0.001,1'\n",
    "cmd += ' -n {}_bestfit'.format(card_name)\n",
    "cmd += ' --saveWorkspace --verbose -1'\n",
    "print cmd\n",
    "os.system(cmd);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical uncertainty\n",
    "cmd = 'combine -M MultiDimFit --algo grid --points=100'\n",
    "cmd += ' --cminDefaultMinimizerStrategy=2 --robustFit 1 --X-rtd MINIMIZER_analytic'\n",
    "cmd += ' -d higgsCombine{}_bestfit.MultiDimFit.mH120.root'.format(card_name)\n",
    "cmd += ' -D ' + histo[histo.keys()[0]]['data'].GetName()\n",
    "cmd += ' --rMin={:.4f} --rMax={:.4f}'.format(c - 5*(c-d), c + 5*(u-c))\n",
    "cmd += ' -n {}_stat'.format(card_name)\n",
    "cmd += ' --snapshotName MultiDimFit'\n",
    "cmd += ' --freezeNuisanceGroups=autoMCStats,allShape'\n",
    "cmd += ' --verbose -1'\n",
    "print cmd\n",
    "os.system(cmd);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC Statistics\n",
    "cmd = 'combine -M MultiDimFit --algo grid --points=100'\n",
    "cmd += ' --cminDefaultMinimizerStrategy=2 --robustFit 1 --X-rtd MINIMIZER_analytic'\n",
    "cmd += ' -d higgsCombine{}_bestfit.MultiDimFit.mH120.root'.format(card_name)\n",
    "cmd += ' -D ' + histo[histo.keys()[0]]['data'].GetName()\n",
    "cmd += ' --rMin={:.4f} --rMax={:.4f}'.format(c - 5*(c-d), c + 5*(u-c))\n",
    "cmd += ' -n {}_MCstat'.format(card_name)\n",
    "cmd += ' --snapshotName MultiDimFit'\n",
    "cmd += ' --freezeNuisanceGroups=autoMCStats'\n",
    "cmd += ' --verbose -1'\n",
    "print cmd\n",
    "os.system(cmd);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'plot1DScan.py higgsCombine{}_nominal.MultiDimFit.mH120.root'.format(card_name)\n",
    "cmd += ' --others'\n",
    "cmd += ' \"higgsCombine{}_MCstat.MultiDimFit.mH120.root:Freeze MC stat:4\"'.format(card_name)\n",
    "cmd += ' \"higgsCombine{}_stat.MultiDimFit.mH120.root:Freeze all:2\"'.format(card_name)\n",
    "cmd += ' --breakdown MCstat,syst,stat'\n",
    "cmd += '; mv scan.png scan_MCstat.png'\n",
    "print cmd\n",
    "os.system(cmd)\n",
    "display(Image(filename='scan_MCstat.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('mv higgsCombine*.root ' + outdir + '/')\n",
    "os.system('mv scan* ' + outdir + '/')\n",
    "os.system('mv combine_logger.out ' + outdir + '/');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the impact plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit first the POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmd = 'combineTool.py -M Impacts --doInitialFit -m 120'\n",
    "cmd += ' --robustFit 1 --X-rtd MINIMIZER_analytic'\n",
    "cmd += ' -d ' + card_location.replace('.txt', '.root')\n",
    "cmd += ' -D ' + histo[histo.keys()[0]]['data'].GetName()\n",
    "cmd += ' --setParameters r={:.2f}'.format(SM_RDst)\n",
    "cmd += ' --setParameterRanges r=0.001,1'\n",
    "cmd += ' -n {}'.format(card_name)\n",
    "cmd += ' --out ' + outdir\n",
    "cmd += ' --verbose -1'\n",
    "os.system(cmd);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Perform a similar scan for each nuisance parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'combineTool.py -M Impacts --doFits -m 120'\n",
    "cmd += ' --robustFit 1 --X-rtd MINIMIZER_analytic'\n",
    "cmd += ' --parallel 20'\n",
    "cmd += ' -d ' + card_location.replace('.txt', '.root')\n",
    "cmd += ' -D ' + histo[histo.keys()[0]]['data'].GetName()\n",
    "cmd += ' -n {}'.format(card_name)\n",
    "cmd += ' --verbose -1'\n",
    "print cmd\n",
    "os.system(cmd);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'combineTool.py -M Impacts -o impacts.json -m 120'\n",
    "cmd += ' -d ' + card_location.replace('.txt', '.root')\n",
    "cmd += ' -n {}'.format(card_name)\n",
    "print cmd\n",
    "os.system(cmd);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename = {\n",
    "'B0pT': 'B_{0} p_{T} spectrum',\n",
    "'B2DstCLNR0':'R_{0} (CLN B#rightarrow D*#ell#nu)',\n",
    "'B2DstCLNR1':'R_{1} (CLN B#rightarrow D*)',\n",
    "'B2DstCLNR2':'R_{2} (CLN B#rightarrow D*)',\n",
    "'B2DstCLNRhoSq':'#rho^{2} (CLN B#rightarrow D*)'\n",
    "}\n",
    "\n",
    "label_dic = {'mu'   : 'B#rightarrow D*#mu#nu',\n",
    "             'tau'  : 'B#rightarrow D*#tau#nu',\n",
    "             'Hc'   : 'B#rightarrow D*H_{c}',\n",
    "             'Dstst': 'B#rightarrow D**#mu#nu'\n",
    "             }\n",
    "for n in processes:\n",
    "    rename[n+'Eff'] = 'Efficiency ' + label_dic[n]\n",
    "json.dump(rename, open('rename.json', 'w'))\n",
    "\n",
    "cmd = 'plotImpacts.py -i impacts.json -o impacts -t rename.json'\n",
    "os.system(cmd)\n",
    "IFrame(\"impacts.pdf\", width=900, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('mv *.root {}/'.format(outdir))\n",
    "os.system('mv impacts.* {}/'.format(outdir))\n",
    "os.system('mv rename.json {}/'.format(outdir))\n",
    "os.system('mv combine_logger.out {}/'.format(outdir));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Goodness of fit test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the observed test stat value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmd = 'combine -M GoodnessOfFit --toysFrequentist --algo=saturated --X-rtd MINIMIZER_analytic'\n",
    "# cmd += ' -d ' + card_location\n",
    "cmd += ' -d results/{cn}/higgsCombine{cn}_bestfit.MultiDimFit.mH120.root'.format(cn=card_name)\n",
    "cmd += ' --snapshotName MultiDimFit --bypassFrequentistFit'\n",
    "cmd += ' -D ' + histo[histo.keys()[0]]['data'].GetName()\n",
    "cmd += ' -n Obs'                                    # Just the output name\n",
    "cmd += ' -t 0'                                      # Don't run toys\n",
    "cmd += ' -s -1'                                     # Random seed\n",
    "cmd += ' --setParameters r={:.2f}'.format(SM_RDst)\n",
    "cmd += ' --setParameterRanges r=0.001,1'\n",
    "cmd += ' --plots --verbose -1'\n",
    "print cmd\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the test stat toy distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'combine -M GoodnessOfFit --toysFrequentist --algo=saturated --X-rtd MINIMIZER_analytic'\n",
    "# cmd += ' -d ' + card_location\n",
    "cmd += ' -d results/{cn}/higgsCombine{cn}_bestfit.MultiDimFit.mH120.root'.format(cn=card_name)\n",
    "cmd += ' --snapshotName MultiDimFit --bypassFrequentistFit'\n",
    "cmd += ' -D ' + histo[histo.keys()[0]]['data'].GetName()\n",
    "cmd += ' -n Toys'                                   # Just the output name\n",
    "cmd += ' -t 50'                                    # Number of toys to run\n",
    "cmd += ' -s -1'                                     # Random seed\n",
    "# cmd += ' --toysFrequentist'\n",
    "# cmd += '--expectSignal=0'                           # Depending on the hypothesis to test. If none, r is fluctruated\n",
    "cmd += ' --setParameters r={:.2f}'.format(SM_RDst)\n",
    "cmd += ' --setParameterRanges r=0.001,1'\n",
    "cmd += ' --plots --verbose -1'\n",
    "print cmd\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine them to get the p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_obs = glob('higgsCombineObs.GoodnessOfFit.*.root')[0]\n",
    "name_toys = glob('higgsCombineToys.GoodnessOfFit.*.root')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ur.open(name_obs)\n",
    "s_obs = f['limit']['limit'].array()[0]\n",
    "\n",
    "f = ur.open(name_toys)\n",
    "s_toys = f['limit']['limit'].array()\n",
    "\n",
    "content, center, _ = plt.hist(s_toys, label='Toys')\n",
    "plt.plot([s_obs, s_obs], [0, np.max(content)], 'm--', label='Observed')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "p_val = np.sum(s_toys > s_obs)/float(s_toys.shape[0])\n",
    "print 'p-value: {:.1f}%'.format(100*p_val)\n",
    "if p_val < 0.01: print p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('mv *.root {}/'.format(outdir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
